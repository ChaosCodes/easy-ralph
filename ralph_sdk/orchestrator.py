"""
Main orchestrator: runs the task pool loop.

Flow:
1. Clarify -> goal.md
2. Initialize -> pool.md + tasks/
3. Loop: Planner -> Worker -> Evaluator -> update files
"""

import asyncio
import re
import shutil
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Optional

from claude_agent_sdk import ClaudeAgentOptions
from rich.console import Console
from rich.prompt import Prompt

from .clarifier import clarify_requirements, clarify_requirements_v2, quick_clarify
from .evaluator import _detect_acceptable_issues_only, _detect_cosmetic_only, evaluate, get_attempt_history, read_adversarial_response
from .logger import SessionLogger, archive_session, format_duration, format_tokens, log_tool_call, stream_query
from .notification import (
    notify_checkpoint,
    notify_complete,
    notify_decision,
    notify_info,
    notify_pivot,
    notify_progress,
    notify_warning,
)
from .planner import Action, PlannerDecision, plan
from .pool import (
    Checkpoint,
    add_checkpoint,
    append_failure_assumptions,
    append_hard_constraint,
    append_to_findings,
    append_to_progress_log,
    clear_handoff_note,
    clear_pivot_recommendation,
    compact_pool,
    count_completed_tasks_with_results,
    create_checkpoint,
    add_task_to_pool,
    ensure_task_files_exist,
    init_task,
    next_task_id,
    task_exists,
    generate_feedback_template,
    generate_handoff_note,
    get_pending_checkpoints,
    goal_exists,
    init_ralph_dir,
    is_waiting_for_user,
    mark_pending_test,
    parse_feedback,
    pool_exists,
    pool_needs_compaction,
    mark_pending_tasks_skipped,
    extract_task_ids_from_pool,
    get_stale_pending_tasks,
    read_checkpoint_manifest,
    read_eval_config_from_goal,
    read_goal,
    read_handoff_note,
    read_pool,
    read_synthesis_kb,
    read_task,
    save_checkpoint_manifest,
    update_kb_experiment_status,
    update_pool_status,
    write_pool,
    write_task,
)
from .prompts import INITIALIZER_SYSTEM_PROMPT
from .synthesizer import synthesize
from .worker import work

console = Console()

# Default target score for task completion
# Tasks scoring below this will continue iterating
DEFAULT_TARGET_SCORE = 95


def parse_target_score_from_goal(cwd: str = ".") -> int:
    """
    Parse target_score from goal.md if specified.

    Looks for patterns like:
    - target_score: 90
    - **Target Score**: 95

    Returns DEFAULT_TARGET_SCORE if not found.
    """
    goal_content = read_goal(cwd)
    if not goal_content:
        return DEFAULT_TARGET_SCORE

    # Look for target_score patterns
    patterns = [
        r'target_score:\s*(\d+)',
        r'\*\*Target Score\*\*:\s*(\d+)',
        r'target score:\s*(\d+)',
    ]

    for pattern in patterns:
        match = re.search(pattern, goal_content, re.IGNORECASE)
        if match:
            return int(match.group(1))

    return DEFAULT_TARGET_SCORE


def enter_waiting_state(
    checkpoints: list[Checkpoint] | list[dict],
    instructions: str,
    cwd: str = ".",
) -> None:
    """
    Enter waiting state for user testing.

    Creates feedback.md template and checkpoint manifest.

    Args:
        checkpoints: List of Checkpoint objects or dicts
        instructions: Testing instructions for user
    """
    # Generate feedback template
    generate_feedback_template(checkpoints, instructions, cwd)

    # Save checkpoint manifest
    save_checkpoint_manifest(
        checkpoints=checkpoints,
        status="waiting_for_user",
        instructions=instructions,
        cwd=cwd,
    )

    append_to_progress_log(
        f"WAITING - Generated {len(checkpoints)} checkpoints for user testing",
        cwd
    )

    console.print("\n[bold yellow]‚ïê‚ïê‚ïê WAITING FOR USER TESTING ‚ïê‚ïê‚ïê[/bold yellow]\n")
    console.print(f"ÁîüÊàê‰∫Ü {len(checkpoints)} ‰∏™ checkpoint ÂæÖÊµãËØï:\n")

    for cp in checkpoints:
        if isinstance(cp, Checkpoint):
            cp_id = cp.id
            proxy_overall = cp.proxy_overall
            proxy_scores = cp.proxy_scores
        else:
            cp_id = cp.get("id", "unknown")
            proxy_overall = cp.get("proxy_overall", 0)
            proxy_scores = cp.get("proxy_scores", {})

        proxy_info = ""
        if proxy_overall > 0:
            proxy_info = f"  [dim](‰ª£ÁêÜÊÄªÂàÜ: {proxy_overall:.0f})[/dim]"
        elif proxy_scores:
            if isinstance(proxy_scores, list):
                scores = ", ".join(f"{ps.metric_name}={ps.score:.0f}" if hasattr(ps, 'metric_name') else f"{ps.get('metric_name', '?')}={ps.get('score', 0):.0f}" for ps in proxy_scores)
            else:
                scores = ", ".join(f"{k}={v}" for k, v in proxy_scores.items())
            proxy_info = f"  [dim](‰ª£ÁêÜÂàÜÊï∞: {scores})[/dim]"

        console.print(f"  - {cp_id}{proxy_info}")

    console.print("\nËØ∑ÁºñËæë [cyan].ralph/feedback.md[/cyan] Â°´ÂÜôÊµãËØïÁªìÊûú")
    console.print("ÂÆåÊàêÂêéËøêË°å [cyan]ralph-sdk resume[/cyan] ÁªßÁª≠\n")


# -----------------------------------------------------------------------------
# Parallel Execution Support
# -----------------------------------------------------------------------------

# Default maximum number of concurrent workers
DEFAULT_MAX_PARALLEL = 3


@dataclass
class ParallelTaskResult:
    """Result of a single task in parallel execution."""
    task_id: str
    task_type: str
    success: bool
    error: Optional[str] = None
    confidence: Optional[str] = None  # For EXPLORE tasks


@dataclass
class ParallelExecutionResult:
    """Aggregated result of parallel task execution."""
    results: list[ParallelTaskResult]
    total_tasks: int
    successful: int
    failed: int

    @property
    def all_succeeded(self) -> bool:
        return self.failed == 0


async def _execute_single_task(
    task_id: str,
    task_type: str,
    cwd: str,
    verbose: bool,
    semaphore: asyncio.Semaphore,
    thinking_budget: int | None = None,
    no_sandbox: bool = False,
) -> ParallelTaskResult:
    """
    Execute a single task with semaphore for concurrency control.

    Args:
        task_id: The task ID to execute
        task_type: Either "EXPLORE" or "IMPLEMENT"
        cwd: Working directory
        verbose: Show detailed output
        semaphore: Semaphore for limiting concurrent executions
        thinking_budget: Token budget for extended thinking

    Returns:
        ParallelTaskResult with execution outcome
    """
    async with semaphore:
        try:
            console.print(f"[cyan]‚ö° Starting parallel task {task_id} ({task_type})...[/cyan]")
            result = await work(task_id, task_type, cwd, verbose=verbose, thinking_budget=thinking_budget, no_sandbox=no_sandbox)
            return ParallelTaskResult(
                task_id=task_id,
                task_type=task_type,
                success=result.success,
                error=result.error,
                confidence=result.confidence,
            )
        except Exception as e:
            console.print(f"[red]‚úó Task {task_id} failed with exception: {e}[/red]")
            return ParallelTaskResult(
                task_id=task_id,
                task_type=task_type,
                success=False,
                error=str(e),
            )


async def execute_parallel_tasks(
    task_ids: list[str],
    cwd: str = ".",
    verbose: bool = False,
    max_parallel: int = DEFAULT_MAX_PARALLEL,
    thinking_budget: int | None = None,
    no_sandbox: bool = False,
) -> ParallelExecutionResult:
    """
    Execute multiple tasks in parallel.

    Uses asyncio.gather to run workers concurrently, with a semaphore
    to limit the maximum number of concurrent executions.

    Args:
        task_ids: List of task IDs to execute
        cwd: Working directory
        verbose: Show detailed output
        max_parallel: Maximum number of concurrent workers

    Returns:
        ParallelExecutionResult with aggregated results
    """
    if not task_ids:
        return ParallelExecutionResult(
            results=[],
            total_tasks=0,
            successful=0,
            failed=0,
        )

    console.print(f"\n[bold magenta]‚ïê‚ïê‚ïê Parallel Execution: {len(task_ids)} tasks (max {max_parallel} concurrent) ‚ïê‚ïê‚ïê[/bold magenta]\n")

    # Create semaphore for concurrency control
    semaphore = asyncio.Semaphore(max_parallel)

    # Determine task types by reading task files
    tasks_with_types = []
    for task_id in task_ids:
        task_content = read_task(task_id, cwd)
        if "Type\nEXPLORE" in task_content or "## Type\nEXPLORE" in task_content:
            task_type = "EXPLORE"
        else:
            task_type = "IMPLEMENT"
        tasks_with_types.append((task_id, task_type))

    # Create coroutines for all tasks
    coroutines = [
        _execute_single_task(task_id, task_type, cwd, verbose, semaphore, thinking_budget=thinking_budget, no_sandbox=no_sandbox)
        for task_id, task_type in tasks_with_types
    ]

    # Run all tasks concurrently (with semaphore limiting actual concurrency)
    results = await asyncio.gather(*coroutines, return_exceptions=True)

    # Process results
    processed_results = []
    successful = 0
    failed = 0

    for (task_id, task_type), result in zip(tasks_with_types, results):
        if isinstance(result, Exception):
            # Handle unexpected exceptions from gather
            processed_results.append(ParallelTaskResult(
                task_id=task_id,
                task_type=task_type,
                success=False,
                error=str(result),
            ))
            failed += 1
        else:
            processed_results.append(result)
            if result.success:
                successful += 1
            else:
                failed += 1

    console.print(f"\n[bold magenta]‚ïê‚ïê‚ïê Parallel Execution Complete: {successful} succeeded, {failed} failed ‚ïê‚ïê‚ïê[/bold magenta]\n")

    return ParallelExecutionResult(
        results=processed_results,
        total_tasks=len(task_ids),
        successful=successful,
        failed=failed,
    )


async def initialize_pool(cwd: str = ".", verbose: bool = False, thinking_budget: int | None = None) -> None:
    """
    Initialize the task pool from goal.md.

    Creates pool.md and initial task files.
    """
    goal = read_goal(cwd)
    if not goal:
        raise ValueError("goal.md not found. Run clarifier first.")

    console.print("\n[yellow]Initializing task pool...[/yellow]\n")

    ralph_dir = str(Path(cwd).resolve() / ".ralph")

    prompt = f"""Goal:
---
{goal}
---

Analyze this goal and create the initial Task Pool.

1. First, explore the codebase to understand the current state.
2. Then create appropriate tasks (start with EXPLORE if uncertain).
3. Write the task table to {ralph_dir}/pool.md
4. Create detailed task files in {ralph_dir}/tasks/

IMPORTANT: All .ralph/ files MUST be written under {ralph_dir}/. Do NOT create .ralph/ directories elsewhere.

Remember: keep initial tasks coarse-grained. It's OK to have only 2-3 tasks.
"""

    init_options = ClaudeAgentOptions(
        system_prompt=INITIALIZER_SYSTEM_PROMPT,
        allowed_tools=[
            "Read", "Write", "Glob", "Grep", "LSP",
            "WebFetch", "WebSearch",
        ],
        permission_mode="acceptEdits",
        max_turns=20,
        cwd=cwd,
    )
    # Initializer default: no thinking (simple task decomposition)
    effective = thinking_budget if thinking_budget is not None else 0
    if effective > 0:
        init_options.max_thinking_tokens = effective

    sr = await stream_query(
        prompt=prompt,
        options=init_options,
        agent_name="initializer",
        emoji="üèóÔ∏è",
        cwd=cwd,
        verbose=verbose,
        show_tools=True,
    )

    console.print("\n[green]‚úì Task pool initialized[/green]")
    return sr.result_stats


def _print_session_summary(logger: SessionLogger, iterations: int) -> None:
    """Print session summary with accumulated stats."""
    elapsed = (datetime.now() - logger._start_time).total_seconds()
    total_tokens = logger.metrics.total_input_tokens + logger.metrics.total_output_tokens

    parts = [format_duration(elapsed)]
    if total_tokens > 0:
        parts.append(format_tokens({
            "input_tokens": logger.metrics.total_input_tokens,
            "output_tokens": logger.metrics.total_output_tokens,
        }))
    parts.append(f"{iterations} iterations")
    if logger.metrics.total_cost_usd > 0:
        parts.append(f"${logger.metrics.total_cost_usd:.2f}")

    console.print(f"\n[dim]Session complete: {' ¬∑ '.join(parts)}[/dim]")


def _create_tasks_from_decision(
    decision: "PlannerDecision",
    logger: "SessionLogger",
    cwd: str,
    default_task_type: str = "IMPLEMENT",
) -> list[str]:
    """
    Create task files from planner decision's new_tasks field.

    Parses the structured task data, auto-assigns IDs if missing,
    creates task files, and adds entries to pool.md.

    Returns list of created task IDs.
    """
    if not decision.new_tasks:
        # Fallback: scan pool.md for task IDs without files
        created = ensure_task_files_exist(cwd)
        if created:
            console.print(f"[dim]Auto-created missing task files: {', '.join(created)}[/dim]")
            for task_id in created:
                logger.log_task_created(task_id, default_task_type, "Auto-created")
        return created

    created = []
    for task_info in decision.new_tasks:
        task_id = task_info.get("task_id") or next_task_id(cwd)
        task_type = task_info.get("task_type", default_task_type)
        title = task_info.get("title", f"Task {task_id}")
        description = task_info.get("description", decision.reason)
        estimated_cost = task_info.get("estimated_cost", "")

        if not task_exists(task_id, cwd):
            init_task(task_id, task_type, title, description, cwd, estimated_cost=estimated_cost)
            add_task_to_pool(task_id, task_type, title, cwd, estimated_cost=estimated_cost)
            created.append(task_id)
            logger.log_task_created(task_id, task_type, title)

    # Also handle any tasks the planner may have added to pool.md directly
    extra = ensure_task_files_exist(cwd)
    for task_id in extra:
        if task_id not in created:
            created.append(task_id)
            logger.log_task_created(task_id, default_task_type, "Auto-created")

    if created:
        console.print(f"[dim]ÂàõÂª∫‰∫Ü‰ªªÂä°: {', '.join(created)}[/dim]")

    return created


def _auto_create_tasks_from_synthesis(
    synthesis_result,
    logger: "SessionLogger",
    cwd: str,
    max_auto_tasks: int = 2,
) -> list[str]:
    """
    Auto-create EXPLORE tasks from synthesis proposed_experiments
    that are marked as diagnostic (P1 priority).

    Only creates up to max_auto_tasks to avoid task explosion.
    Also updates KB experiment status: Proposed ‚Üí Executing.
    Returns list of created task IDs.
    """
    if not synthesis_result.proposed_experiments:
        return []

    created = []
    for exp in synthesis_result.proposed_experiments:
        if len(created) >= max_auto_tasks:
            break
        if not exp.is_diagnostic:
            continue

        task_id = next_task_id(cwd)
        title = f"[Auto-Synthesis] {exp.name}"
        description = (
            f"**Insight tested**: {exp.insight_tested}\n"
            f"**Method**: {exp.method}\n"
            f"**Expected outcome**: {exp.expected_outcome}\n"
            f"**Differs from tried**: {exp.differs_from_tried}"
        )

        if not task_exists(task_id, cwd):
            init_task(task_id, "EXPLORE", title, description, cwd)
            add_task_to_pool(task_id, "EXPLORE", title, cwd)
            created.append(task_id)
            logger.log_task_created(task_id, "EXPLORE", title)

            # Update KB: move experiment from Proposed ‚Üí Executing
            # Try to find experiment ID from the name
            exp_id_match = re.search(r"(E\d+)", exp.name)
            if exp_id_match:
                update_kb_experiment_status(
                    experiment_id=exp_id_match.group(1),
                    from_section="Proposed",
                    to_section="Executing",
                    extra_cols=f"| {task_id} | {exp.name}",
                    cwd=cwd,
                )

    if created:
        console.print(f"[dim]üî¨ Auto-created {len(created)} diagnostic task(s) from synthesis: {', '.join(created)}[/dim]")

    return created


def _handle_pivot_tail(
    decision: "PlannerDecision",
    logger: "SessionLogger",
    iteration: int,
    progress_msg: str,
    cwd: str,
) -> None:
    """Shared tail logic for all pivot actions: log progress, clear marker, log iteration."""
    append_to_progress_log(progress_msg, cwd)
    clear_pivot_recommendation(decision.target, cwd)
    logger.log_iteration_end(iteration, decision.action.value)


def _update_kb_on_task_complete(task_id: str, result_summary: str, cwd: str = ".") -> None:
    """
    When a task completes, check if it corresponds to an Executing experiment in KB.
    If so, move the experiment to Completed with result.
    """
    kb = read_synthesis_kb(cwd)
    if task_id not in kb:
        return

    # Find experiment ID associated with this task in Executing section
    executing_match = re.search(
        r"### Executing\s*\n(.*?)(?=\n### |\n## |\Z)",
        kb,
        re.DOTALL,
    )
    if not executing_match:
        return

    for line in executing_match.group(1).split("\n"):
        if task_id in line and line.strip().startswith("|"):
            # Extract experiment ID from the row
            cols = [c.strip() for c in line.split("|")]
            cols = [c for c in cols if c]
            if cols and re.match(r"E\d+", cols[0]):
                update_kb_experiment_status(
                    experiment_id=cols[0],
                    from_section="Executing",
                    to_section="Completed",
                    extra_cols=f"| {result_summary}",
                    cwd=cwd,
                )
                return


def _clean_ralph_dir(cwd: str = ".") -> None:
    """Clean .ralph/ directory contents, preserving history/."""
    ralph_dir = Path(cwd) / ".ralph"
    if ralph_dir.exists():
        for item in ralph_dir.iterdir():
            if item.name != "history":
                if item.is_dir():
                    shutil.rmtree(item)
                else:
                    item.unlink()


async def run(
    goal: str,
    cwd: str = ".",
    max_iterations: int = 30,
    skip_clarify: bool = False,
    verbose: bool = False,
    max_parallel: int = DEFAULT_MAX_PARALLEL,
    clarify_mode: str = "auto",
    target_score: int = None,
    explore_mode: bool = False,
    thinking_budget: int | None = None,
    no_sandbox: bool = False,
    synthesis_interval: int = 3,
) -> bool:
    """
    Run the full task pool loop.

    Args:
        goal: The user's goal/feature request
        cwd: Working directory
        max_iterations: Maximum number of iterations
        skip_clarify: If True, skip interactive clarification
        verbose: Show detailed output
        max_parallel: Maximum number of concurrent workers for PARALLEL_EXECUTE
        clarify_mode: Clarification mode - "auto" | "ask" | "explore"
            - auto: Automatically choose based on request clarity
            - ask: Use traditional Q&A mode (Clarifier v1)
            - explore: Use explore+propose mode (Clarifier v2)
        target_score: Target quality score (0-100). Tasks scoring below this
            will continue iterating. If None, parsed from goal.md or defaults to 95.
        explore_mode: If True, don't DONE until user explicitly says stop.
            Continues exploring alternatives even after tasks complete.
        thinking_budget: Token budget for extended thinking. None=agent defaults,
            0=off for all agents, N=use N for all agents.
        synthesis_interval: Run synthesis after every N new completed tasks.
            0 disables periodic synthesis. Default 3.

    Returns:
        True if goal was completed, False if max iterations reached
    """
    init_ralph_dir(cwd)

    # Detect session conflict: new goal vs existing session
    if goal and goal_exists(cwd):
        existing_goal = read_goal(cwd)
        new_first = goal.strip().split('\n')[0].strip().lower()
        existing_first = existing_goal.strip().split('\n')[0].strip().lower()

        if new_first != existing_first:
            console.print(f"[yellow]‚ö† Ê£ÄÊµãÂà∞Â∑≤Êúâ sessionÔºàÁõÆÊ†á: {existing_first[:60]}Ôºâ[/yellow]")
            console.print(f"[yellow]  Êñ∞ÁõÆÊ†á: {new_first[:60]}[/yellow]\n")

            choice = Prompt.ask(
                "Â¶Ç‰ΩïÂ§ÑÁêÜÔºü",
                choices=["archive", "resume", "abort"],
                default="archive",
            )

            if choice == "archive":
                archive_path = archive_session(cwd, keep_current=False)
                if archive_path:
                    console.print(f"[green]‚úì Êóß session Â∑≤ÂΩíÊ°£Âà∞ {archive_path}[/green]")
                _clean_ralph_dir(cwd)
                init_ralph_dir(cwd)
            elif choice == "resume":
                console.print("[dim]ÂøΩÁï•Êñ∞ÁõÆÊ†áÔºåÁªßÁª≠Êóß session[/dim]")
            else:  # abort
                console.print("[yellow]Â∑≤ÂèñÊ∂à[/yellow]")
                return False

    # Initialize session logger
    logger = SessionLogger(cwd)
    logger.log_session_start(goal)

    # Phase 1: Clarify
    if not goal_exists(cwd):
        console.print("\n[bold]Phase 1: Clarifying requirements[/bold]\n")
        if skip_clarify:
            await quick_clarify(goal, cwd)
        elif clarify_mode == "explore":
            await clarify_requirements_v2(goal, cwd, mode="explore", verbose=verbose)
        elif clarify_mode == "ask":
            await clarify_requirements(goal, cwd, verbose=verbose)
        else:  # auto mode
            await clarify_requirements_v2(goal, cwd, mode="auto", verbose=verbose)
    else:
        console.print("\n[dim]Goal already exists, skipping clarification[/dim]")

    # Phase 2: Initialize pool
    if not pool_exists(cwd):
        console.print("\n[bold]Phase 2: Initializing task pool[/bold]\n")
        init_stats = await initialize_pool(cwd, verbose=verbose, thinking_budget=thinking_budget)
        if init_stats:
            logger.log_query_stats(init_stats)
        if not pool_exists(cwd):
            raise RuntimeError(
                f"Initializer completed but pool.md was not created at "
                f"{Path(cwd).resolve() / '.ralph' / 'pool.md'}. "
                f"Check tool_calls.jsonl for Write tool paths."
            )
    else:
        console.print("\n[dim]Pool already exists, skipping initialization[/dim]")

    # Determine target score
    if target_score is None:
        target_score = parse_target_score_from_goal(cwd)
    console.print(f"[dim]Target score: {target_score}/100[/dim]")

    # Show explore mode status
    if explore_mode:
        console.print("[bold magenta]üîç Explore mode: ON[/bold magenta]")
        console.print("[dim]Will continue exploring until user says stop in feedback.md[/dim]")

    # Phase 3: Iteration loop
    console.print("\n[bold]Phase 3: Executing tasks[/bold]\n")

    # Adaptive synthesis: track completions since last synthesis
    tasks_at_last_synthesis = 0

    consecutive_skips = 0
    consecutive_empty_creates = 0

    # Auto-continue: skip planner when last eval passed but below target (Exp 3)
    last_iter_state: dict = {}  # {action, verdict, score, task_id, should_pivot}

    # Adversarial skip: skip adversarial testing after consecutive clean rounds (Exp 4)
    adversarial_clean_count: dict[str, int] = {}  # task_id ‚Üí consecutive clean rounds

    # Cosmetic stagnation: terminate when consecutive cosmetic-only rounds (Improvement 3)
    cosmetic_stagnation_count: dict[str, int] = {}  # task_id ‚Üí consecutive cosmetic-only rounds

    for i in range(1, max_iterations + 1):
      try:
        # Build cumulative stats for iteration header
        elapsed = (datetime.now() - logger._start_time).total_seconds()
        total_tokens = logger.metrics.total_input_tokens + logger.metrics.total_output_tokens
        iter_stats_parts = []
        if elapsed > 60:
            iter_stats_parts.append(format_duration(elapsed))
        if total_tokens > 0:
            iter_stats_parts.append(format_tokens({
                "input_tokens": logger.metrics.total_input_tokens,
                "output_tokens": logger.metrics.total_output_tokens,
            }))
        if logger.metrics.total_cost_usd > 0:
            iter_stats_parts.append(f"${logger.metrics.total_cost_usd:.2f}")
        iter_suffix = f" ({' ¬∑ '.join(iter_stats_parts)})" if iter_stats_parts else ""
        console.print(f"\n[bold cyan]‚ïê‚ïê‚ïê Iteration {i}/{max_iterations}{iter_suffix} ‚ïê‚ïê‚ïê[/bold cyan]\n")
        logger.log_iteration_start(i, max_iterations)

        # Context compaction: compress pool.md if it's grown too large
        if pool_needs_compaction(cwd):
            console.print("[dim]üì¶ Pool.md exceeds threshold, compacting...[/dim]")
            try:
                compacted = await compact_pool(cwd)
                if compacted:
                    console.print("[dim]üì¶ Pool.md compacted successfully[/dim]")
                    append_to_progress_log("COMPACTION - pool.md compressed to reduce context noise", cwd)
            except Exception as e:
                console.print(f"[dim]üì¶ Compaction failed (non-critical): {e}[/dim]")

        # Stale pending detection: flag tasks that have been pending too long
        stale_tasks = get_stale_pending_tasks(cwd, threshold=5)
        if stale_tasks:
            # Only write finding if not already flagged (dedup)
            pool_content_check = read_pool(cwd)
            unflagged = [t for t in stale_tasks if f"[STALE_PENDING] {t}" not in pool_content_check]
            if unflagged:
                append_to_findings(
                    f"**[STALE_PENDING]** Tasks idle for 5+ iterations: {', '.join(unflagged)}. "
                    "Must EXECUTE, DELETE, or address these before creating new tasks.",
                    cwd,
                )
                console.print(f"[yellow]‚ö† Stale pending tasks detected: {', '.join(unflagged)}[/yellow]")

        # Adaptive synthesis: trigger after N new completed tasks
        # Interval decreases as the experiment pool grows (more data ‚Üí synthesize more often)
        if synthesis_interval > 0:
            completed_now = count_completed_tasks_with_results(cwd, task_type="IMPLEMENT")
            effective_interval = max(1, synthesis_interval - (completed_now // 6))
            if completed_now - tasks_at_last_synthesis >= effective_interval:
                console.print("[dim]üî¨ Running periodic insight synthesis...[/dim]")
                try:
                    synthesis_result = await synthesize(cwd, verbose=verbose, thinking_budget=thinking_budget)
                    if synthesis_result.insights:
                        console.print(f"[dim]üî¨ {len(synthesis_result.insights)} insights extracted[/dim]")
                    if synthesis_result.result_stats:
                        logger.log_query_stats(synthesis_result.result_stats)
                    logger.log_synthesis(
                        insight_count=len(synthesis_result.insights),
                        experiment_count=len(synthesis_result.proposed_experiments),
                        trigger="periodic",
                    )
                    # Auto-create diagnostic tasks from synthesis
                    _auto_create_tasks_from_synthesis(synthesis_result, logger, cwd)
                except Exception as e:
                    console.print(f"[dim]üî¨ Synthesis failed (non-critical): {e}[/dim]")
                tasks_at_last_synthesis = completed_now

        # Auto-continue: skip planner when last eval passed but below target (Exp 3)
        auto_continue = (
            last_iter_state.get("action") == "execute"
            and last_iter_state.get("verdict") == "passed"
            and last_iter_state.get("score", 0) < target_score
            and not last_iter_state.get("should_pivot", False)
        )

        if auto_continue:
            task_id = last_iter_state["task_id"]
            console.print(f"[dim]‚è≠ Auto-continue: {task_id} score {last_iter_state['score']:.0f} < {target_score}[/dim]")
            decision = PlannerDecision(
                action=Action.EXECUTE,
                target=task_id,
                reason=f"Auto-continue: score {last_iter_state['score']:.0f}/{target_score}",
            )
            logger.log_planner_decision(
                action=decision.action.value,
                target=decision.target,
                reason=decision.reason,
            )
        else:
            # Planner decides next action
            console.print("[yellow]Planner deciding...[/yellow]")
            decision = await plan(cwd, verbose=verbose, explore_mode=explore_mode, thinking_budget=thinking_budget)

            # Log planner stats
            if decision.result_stats:
                logger.log_query_stats(decision.result_stats)

        console.print(f"[bold]Action:[/bold] {decision.action.value}")
        if decision.target:
            console.print(f"[bold]Target:[/bold] {decision.target}")
        if decision.task_ids:
            console.print(f"[bold]Tasks:[/bold] {', '.join(decision.task_ids)}")
        console.print(f"[bold]Reason:[/bold] {decision.reason}")

        if not auto_continue:
            logger.log_planner_decision(
                action=decision.action.value,
                target=decision.target,
                reason=decision.reason,
            )

        # Reset consecutive skip counter for non-SKIP actions
        if decision.action != Action.SKIP:
            consecutive_skips = 0

        # Reset last_iter_state for non-EXECUTE actions (Exp 3)
        if decision.action != Action.EXECUTE:
            last_iter_state = {}

        # Handle DONE
        if decision.action == Action.DONE:
            # Check for unprocessed PIVOT recommendations (P0: block DONE if pivot pending)
            pool_content = read_pool(cwd)
            if "[PIVOT_RECOMMENDED]" in pool_content:
                console.print("[yellow]‚ö†Ô∏è Êúâ PIVOT Âª∫ËÆÆÊú™Â§ÑÁêÜÔºåÁªßÁª≠Ëø≠‰ª£[/yellow]")
                console.print("[dim]Â¶ÇÈúÄÂº∫Âà∂ÁªìÊùüÔºåËØ∑Âú® feedback.md ‰∏≠ÂÜô next_step: stop[/dim]")
                logger.log_iteration_end(i, "DONE_BLOCKED", reason="Unprocessed pivot recommendation")
                append_to_progress_log(f"DONE_BLOCKED - Unprocessed pivot recommendation", cwd)
                continue

            # P0: Check if any non-skipped task scored below target_score
            task_ids = extract_task_ids_from_pool(cwd)
            below_target = []
            for tid in task_ids:
                task_content = read_task(tid, cwd)
                if not task_content:
                    continue
                # Skip tasks with status: skipped/pending
                content_lower = task_content.lower()
                status_match = re.search(r"## status[:\s]+(?:[^\w\s]\s*)?(\w+)", content_lower)
                if status_match and status_match.group(1) in ("skipped", "pending"):
                    continue
                # Check latest score
                _, prev_scores = get_attempt_history(tid, cwd)
                if prev_scores and prev_scores[-1] < target_score:
                    below_target.append((tid, prev_scores[-1]))
            if below_target:
                for tid, score in below_target:
                    console.print(f"[yellow]\u26a0\ufe0f {tid} latest score {score:.0f} < target {target_score}, continuing[/yellow]")
                logger.log_iteration_end(i, "DONE_BLOCKED", reason=f"Tasks below target: {below_target}")
                append_to_progress_log(f"DONE_BLOCKED - Tasks below target_score: {below_target}", cwd)
                continue

            # Check explore mode - need user confirmation to stop
            if explore_mode:
                feedback = parse_feedback(cwd)
                user_wants_stop = feedback and feedback.get("next_step") == "stop"
                if not user_wants_stop:
                    console.print("[yellow]‚ö†Ô∏è Êé¢Á¥¢Ê®°ÂºèÔºö‰∏çÂÖÅËÆ∏ DONEÔºåÂàõÂª∫Êñ∞Êé¢Á¥¢‰ªªÂä°[/yellow]")
                    console.print("[dim]Â¶ÇÈúÄÂÅúÊ≠¢ÔºåËØ∑Âú® feedback.md ‰∏≠ÂÜô next_step: stop[/dim]")
                    logger.log_iteration_end(i, "DONE_BLOCKED", reason="Explore mode - user stop not confirmed")
                    append_to_progress_log(f"DONE_BLOCKED - Explore mode active, waiting for user stop signal", cwd)
                    continue

            console.print("\n[bold green]‚úì Goal completed![/bold green]")
            _print_session_summary(logger, i)
            append_to_progress_log(f"DONE - Goal completed after {i} iterations", cwd)
            # Update pool status and mark pending tasks as skipped
            update_pool_status("COMPLETED", cwd)
            skipped = mark_pending_tasks_skipped(
                f"Superseded ‚Äî goal completed after {i} iterations", cwd
            )
            if skipped:
                console.print(f"[dim]Marked {len(skipped)} pending task(s) as skipped: {', '.join(skipped)}[/dim]")
            generate_handoff_note(cwd)  # Save state for potential future resume
            logger.log_session_end(success=True, reason=f"Completed after {i} iterations")
            return True

        # Handle CREATE / DECOMPOSE / DELETE
        if decision.action in (Action.CREATE, Action.DECOMPOSE, Action.DELETE):
            append_to_progress_log(
                f"{decision.action.value.upper()} - {decision.reason}",
                cwd
            )
            # Create task files from planner's new_tasks data
            if decision.action in (Action.CREATE, Action.DECOMPOSE):
                created = _create_tasks_from_decision(decision, logger, cwd)
                if not created:
                    consecutive_empty_creates += 1
                    if consecutive_empty_creates >= 3:
                        append_to_findings(
                            "**[PLANNER_BUG]** CREATE returned empty new_tasks "
                            f"{consecutive_empty_creates} times in a row. "
                            "Breaking CREATE loop ‚Äî planner should EXECUTE or EXPLORE instead.",
                            cwd,
                        )
                        console.print(
                            f"[red]‚ö† CREATE dead loop detected "
                            f"({consecutive_empty_creates} empty CREATEs). "
                            f"Wrote [PLANNER_BUG] finding.[/red]"
                        )
                        consecutive_empty_creates = 0
                else:
                    consecutive_empty_creates = 0
            logger.log_iteration_end(i, decision.action.value)
            continue

        # Handle SYNTHESIZE
        if decision.action == Action.SYNTHESIZE:
            console.print("\n[yellow]üî¨ Running insight synthesis (planner-requested)...[/yellow]\n")
            syn_insights = 0
            syn_experiments = 0
            try:
                synthesis_result = await synthesize(cwd, verbose=verbose, thinking_budget=thinking_budget)
                syn_insights = len(synthesis_result.insights)
                syn_experiments = len(synthesis_result.proposed_experiments)

                if synthesis_result.insights:
                    console.print(f"[green]‚úì {syn_insights} insights extracted[/green]")
                    for ins in synthesis_result.insights:
                        console.print(f"  [dim]- {ins.observation[:100]}[/dim]")
                else:
                    console.print("[dim]No new insights found[/dim]")

                if synthesis_result.proposed_experiments:
                    console.print(f"[dim]üî¨ {syn_experiments} experiments proposed[/dim]")

                if synthesis_result.result_stats:
                    logger.log_query_stats(synthesis_result.result_stats)
                logger.log_synthesis(
                    insight_count=syn_insights,
                    experiment_count=syn_experiments,
                    trigger="planner",
                )
                # Auto-create diagnostic tasks from synthesis
                _auto_create_tasks_from_synthesis(synthesis_result, logger, cwd)
            except Exception as e:
                console.print(f"[red]‚úó Synthesis failed: {e}[/red]")
                logger.log_error(f"Synthesis failed: {e}")

            append_to_progress_log(
                f"SYNTHESIZE - {syn_insights} insights, {syn_experiments} experiments proposed",
                cwd,
            )
            logger.log_iteration_end(i, decision.action.value)
            continue

        # Handle EXPLORE
        if decision.action == Action.EXPLORE:
            if not decision.target:
                console.print("[red]Error: EXPLORE requires a target task[/red]")
                logger.log_error("EXPLORE requires a target task")
                continue

            console.print(f"\n[yellow]Exploring {decision.target}...[/yellow]\n")
            result = await work(decision.target, "EXPLORE", cwd, verbose=verbose, thinking_budget=thinking_budget, no_sandbox=no_sandbox)

            # Log worker stats
            if result.result_stats:
                logger.log_query_stats(result.result_stats)

            # Log worker result
            logger.log_worker_complete(
                task_id=decision.target,
                task_type="EXPLORE",
                success=result.success,
                error=result.error,
            )

            if result.success:
                console.print(f"[green]‚úì Exploration complete[/green]")
                if result.confidence:
                    console.print(f"[dim]Confidence: {result.confidence}[/dim]")
            else:
                console.print(f"[red]‚úó Exploration failed: {result.error}[/red]")

            append_to_progress_log(
                f"EXPLORE {decision.target} - {'success' if result.success else 'failed'}"
                + (f" (confidence: {result.confidence})" if result.confidence else ""),
                cwd
            )
            logger.log_iteration_end(i, decision.action.value, success=result.success)
            continue

        # Handle PARALLEL_EXECUTE
        if decision.action == Action.PARALLEL_EXECUTE:
            if not decision.task_ids:
                console.print("[red]Error: PARALLEL_EXECUTE requires TASK_IDS[/red]")
                logger.log_error("PARALLEL_EXECUTE requires TASK_IDS")
                continue

            if len(decision.task_ids) < 2:
                console.print("[yellow]Warning: Only one task provided, using regular EXECUTE[/yellow]")
                # Fall through to single task execution
                decision.target = decision.task_ids[0]
                decision.action = Action.EXECUTE
                # Don't continue - let it fall through to EXECUTE handler

            else:
                # Execute tasks in parallel
                parallel_result = await execute_parallel_tasks(
                    task_ids=decision.task_ids,
                    cwd=cwd,
                    verbose=verbose,
                    max_parallel=max_parallel,
                    thinking_budget=thinking_budget,
                    no_sandbox=no_sandbox,
                )

                # Log each task result
                for task_result in parallel_result.results:
                    logger.log_worker_complete(
                        task_id=task_result.task_id,
                        task_type=task_result.task_type,
                        success=task_result.success,
                        error=task_result.error,
                    )

                # Log aggregate result
                status_parts = []
                for task_result in parallel_result.results:
                    status = "‚úì" if task_result.success else "‚úó"
                    status_parts.append(f"{task_result.task_id}:{status}")

                append_to_progress_log(
                    f"PARALLEL_EXECUTE [{', '.join(decision.task_ids)}] - "
                    f"{parallel_result.successful}/{parallel_result.total_tasks} succeeded "
                    f"({'; '.join(status_parts)})",
                    cwd
                )

                # Display summary
                if parallel_result.all_succeeded:
                    console.print(f"[green]‚úì All {parallel_result.total_tasks} parallel tasks completed successfully[/green]")
                else:
                    console.print(f"[yellow]‚ö† Parallel execution: {parallel_result.successful} succeeded, {parallel_result.failed} failed[/yellow]")
                    for task_result in parallel_result.results:
                        if not task_result.success:
                            console.print(f"  [red]‚úó {task_result.task_id}: {task_result.error}[/red]")

                logger.log_iteration_end(i, decision.action.value, success=parallel_result.all_succeeded)
                continue

        # Handle FORK
        if decision.action == Action.FORK:
            if not decision.fork_approaches:
                console.print("[red]Error: FORK requires fork_approaches[/red]")
                logger.log_error("FORK requires fork_approaches")
                continue

            if not decision.target:
                console.print("[red]Error: FORK requires a target (base session task)[/red]")
                logger.log_error("FORK requires a target")
                continue

            console.print(f"\n[magenta]üîÄ FORK: Trying {len(decision.fork_approaches)} approaches in parallel[/magenta]")
            for idx, approach in enumerate(decision.fork_approaches, 1):
                console.print(f"  [dim]{idx}. {approach[:100]}[/dim]")

            # Fork each approach as a parallel EXPLORE task
            fork_results = []
            for approach in decision.fork_approaches:
                result = await work(decision.target, "EXPLORE", cwd, verbose=verbose, thinking_budget=thinking_budget, no_sandbox=no_sandbox)
                fork_results.append(result)

            # Log results
            success_count = sum(1 for r in fork_results if r.success)
            append_to_progress_log(
                f"FORK {decision.target} - {len(decision.fork_approaches)} approaches tried, "
                f"{success_count} succeeded",
                cwd,
            )

            for r in fork_results:
                if r.result_stats:
                    logger.log_query_stats(r.result_stats)

            logger.log_iteration_end(i, decision.action.value, success=success_count > 0)
            continue

        # Handle EXECUTE
        if decision.action == Action.EXECUTE:
            if not decision.target:
                console.print("[red]Error: EXECUTE requires a target task[/red]")
                logger.log_error("EXECUTE requires a target task")
                continue

            # Auto-state tracking for auto-continue (Exp 3)
            _exec_auto_state: dict = {}

            console.print(f"\n[yellow]Executing {decision.target}...[/yellow]\n")

            # Read adversarial findings from the most recent evaluation (if any)
            adv_findings = None
            audits_path = Path(cwd) / ".ralph" / "audits"
            if audits_path.exists():
                attempt_number, _ = get_attempt_history(decision.target, cwd)
                for att in range(attempt_number, 0, -1):
                    findings_file = audits_path / f"adversarial_{decision.target}_{att}.md"
                    if findings_file.exists():
                        content = findings_file.read_text()
                        if "No adversarial issues found" not in content:
                            adv_findings = content
                        break

            result = await work(
                decision.target, "IMPLEMENT", cwd,
                verbose=verbose,
                thinking_budget=thinking_budget,
                adversarial_findings=adv_findings,
                no_sandbox=no_sandbox,
            )

            # Log worker stats
            if result.result_stats:
                logger.log_query_stats(result.result_stats)

            # Log worker result
            logger.log_worker_complete(
                task_id=decision.target,
                task_type="IMPLEMENT",
                success=result.success,
                error=result.error,
            )

            if result.success:
                console.print(f"[green]‚úì Execution complete, evaluating...[/green]\n")

                # Skip reviewer ‚Äî go directly to Evaluator
                # (Worker's prompt already requires tests to pass before claiming done;
                #  Evaluator runs pytest + checks all acceptance criteria)

                # Run quality evaluation
                console.print(f"\n[yellow]Evaluating quality...[/yellow]")

                # Get attempt history for pivot detection
                attempt_number, previous_scores = get_attempt_history(decision.target, cwd)

                # Read Worker's adversarial response from previous round (if any)
                prev_adv_response = read_adversarial_response(
                    decision.target, attempt_number - 1, cwd
                ) if attempt_number > 1 else None

                # Skip adversarial testing if 2+ consecutive clean rounds (Exp 4)
                skip_adv = adversarial_clean_count.get(decision.target, 0) >= 2
                if skip_adv:
                    console.print(f"[dim]‚è≠ Skipping adversarial testing ({adversarial_clean_count[decision.target]} consecutive clean rounds)[/dim]")

                try:
                    eval_result = await evaluate(
                        decision.target,
                        cwd=cwd,
                        verbose=verbose,
                        previous_scores=previous_scores,
                        attempt_number=attempt_number,
                        thinking_budget=thinking_budget,
                        previous_adversarial_responses=prev_adv_response,
                        skip_adversarial=skip_adv,
                    )
                except Exception as eval_exc:
                    console.print(f"[red]‚úó Evaluator crashed: {eval_exc}[/red]")
                    console.print("[yellow]Skipping evaluation, will retry next iteration[/yellow]")
                    append_to_progress_log(
                        f"EXECUTE {decision.target} - EVAL_CRASH: {str(eval_exc)[:200]}",
                        cwd
                    )
                    logger.log_iteration_end(i, decision.action.value, success=True)
                    generate_handoff_note(cwd)
                    continue

                # Auto-retry on infra failure (up to 2 retries)
                MAX_EVAL_RETRIES = 2
                eval_retry = 0
                while eval_result.is_infra_failure and eval_retry < MAX_EVAL_RETRIES:
                    eval_retry += 1
                    console.print(f"[yellow]‚ö† EVAL_INFRA_FAILURE detected, retrying evaluation ({eval_retry}/{MAX_EVAL_RETRIES})...[/yellow]")
                    eval_result = await evaluate(
                        decision.target,
                        cwd=cwd,
                        verbose=verbose,
                        previous_scores=previous_scores,
                        attempt_number=attempt_number,
                        thinking_budget=thinking_budget,
                        previous_adversarial_responses=prev_adv_response,
                        skip_adversarial=skip_adv,
                    )

                # Update adversarial clean count (Exp 4)
                if eval_result.adversarial_findings:
                    adversarial_clean_count[decision.target] = 0
                else:
                    adversarial_clean_count[decision.target] = adversarial_clean_count.get(decision.target, 0) + 1

                # Log evaluator stats
                if eval_result.result_stats:
                    logger.log_query_stats(eval_result.result_stats)

                # Log evaluation (mark infra failures distinctly)
                if eval_result.is_infra_failure:
                    logger.log_evaluation(
                        task_id=decision.target,
                        passed=False,
                        score=0,
                        issues=["EVAL_INFRA_FAILURE: evaluator did not produce valid output after retries"],
                    )
                    console.print(f"[red]‚úó EVAL_INFRA_FAILURE: evaluator failed to produce output after {MAX_EVAL_RETRIES} retries[/red]")
                    append_to_progress_log(
                        f"EXECUTE {decision.target} - EVAL_INFRA_FAILURE (evaluator did not produce output, skipping this evaluation)",
                        cwd
                    )
                    logger.log_iteration_end(i, decision.action.value, success=True)
                    continue
                else:
                    logger.log_evaluation(
                        task_id=decision.target,
                        passed=eval_result.overall_passed,
                        score=eval_result.overall_score,
                        issues=eval_result.issues,
                    )

                # P0: If Evaluator suggests pivot, write to pool.md for Planner to see
                if eval_result.should_pivot:
                    append_to_findings(
                        f"**[PIVOT_RECOMMENDED]** {decision.target}: {eval_result.pivot_reason}",
                        cwd
                    )
                    console.print(f"[yellow]‚ö†Ô∏è Evaluator Âª∫ËÆÆËΩ¨Âêë: {eval_result.pivot_reason}[/yellow]")

                # Check if user testing is needed
                if eval_result.needs_user_testing:
                    eval_config = read_eval_config_from_goal(cwd)
                    pending_metrics = eval_result.get_pending_manual_metrics()

                    # Create checkpoint with proper structure
                    checkpoint = create_checkpoint(
                        task_id=decision.target,
                        path=f"checkpoints/{decision.target}",
                        artifact_type="code",
                        description=f"Implementation of {decision.target}",
                        cwd=cwd,
                    )

                    # Add proxy scores from evaluation
                    for mr in eval_result.metrics:
                        if mr.proxy_score is not None:
                            checkpoint.add_proxy_score(
                                metric_name=mr.metric.name,
                                score=mr.proxy_score,
                                target=mr.metric.target,
                                notes=mr.proxy_notes,
                            )
                        elif not mr.pending_manual and mr.score is not None:
                            checkpoint.add_proxy_score(
                                metric_name=mr.metric.name,
                                score=mr.score,
                                target=mr.metric.target,
                                notes="Auto-evaluated",
                            )

                    # Generate test instructions
                    instructions = "ËØ∑ÊµãËØï‰ª•‰∏ãÊåáÊ†á:\n"
                    for mr in pending_metrics:
                        instructions += f"- {mr.metric.name}: {mr.metric.description}\n"
                        if mr.metric.target:
                            instructions += f"  ÁõÆÊ†á: {mr.metric.target}\n"
                        if mr.proxy_score is not None:
                            instructions += f"  ‰ª£ÁêÜÂàÜÊï∞: {mr.proxy_score:.0f}\n"

                    # Decide whether to pause based on batch preference
                    batch_pref = eval_config.get("batch_preference", "")

                    # Always add checkpoint first
                    add_checkpoint(checkpoint, cwd)

                    if "‰∏Ä‰∏™‰∏Ä‰∏™" in batch_pref or not batch_pref:
                        mark_pending_test(decision.target, cwd)
                        notify_checkpoint(
                            checkpoint_id=checkpoint.id,
                            task_id=decision.target,
                            proxy_score=checkpoint.proxy_overall,
                            description=f"{len(pending_metrics)} È°πÊåáÊ†áÂæÖÁî®Êà∑ÊµãËØï",
                        )
                    else:
                        notify_checkpoint(
                            checkpoint_id=checkpoint.id,
                            task_id=decision.target,
                            proxy_score=checkpoint.proxy_overall,
                            description=f"{len(pending_metrics)} È°πÊåáÊ†áÂæÖÊâπÈáèÊµãËØï",
                        )

                    # Check if we've accumulated enough for batch testing
                    pending = get_pending_checkpoints(cwd)
                    batch_size = int(eval_config.get("batch_size", 3) or 3)
                    if len(pending) >= batch_size:
                        console.print(f"\n[yellow]Batch size reached ({len(pending)} checkpoints)[/yellow]")
                        enter_waiting_state(
                            checkpoints=pending,
                            instructions=instructions,
                            cwd=cwd,
                        )
                        generate_handoff_note(cwd)  # Save state for resume
                        logger.log_session_end(success=False, reason=f"Waiting for batch testing ({len(pending)} checkpoints)")
                        return False

                # Acceptable issues stagnation termination (Improvement 3+)
                # Accept if only COSMETIC or ACCEPTABLE_TRADEOFF issues remain
                is_acceptable = _detect_acceptable_issues_only(eval_result.issues)
                if is_acceptable and eval_result.overall_score >= target_score:
                    cosmetic_stagnation_count[decision.target] = cosmetic_stagnation_count.get(decision.target, 0) + 1
                else:
                    cosmetic_stagnation_count[decision.target] = 0

                cosmetic_stagnated = cosmetic_stagnation_count.get(decision.target, 0) >= 2

                if cosmetic_stagnated:
                    console.print(f"[green]‚úì Task {decision.target} accepted despite minor issues "
                                  f"(score: {eval_result.overall_score:.0f}, {cosmetic_stagnation_count[decision.target]} consecutive rounds with only acceptable issues)[/green]")
                    append_to_progress_log(
                        f"EXECUTE {decision.target} - ACCEPTABLE_ISSUES_STAGNATION_ACCEPTED "
                        f"(score: {eval_result.overall_score:.0f}/{target_score}, only COSMETIC/ACCEPTABLE_TRADEOFF issues for {cosmetic_stagnation_count[decision.target]} rounds)",
                        cwd
                    )
                elif eval_result.overall_passed and eval_result.overall_score >= target_score:
                    console.print(f"[green]‚úì Task {decision.target} completed (score: {eval_result.overall_score:.0f}/{target_score})[/green]")
                    append_to_progress_log(
                        f"EXECUTE {decision.target} - PASSED (score: {eval_result.overall_score:.0f}/{target_score})",
                        cwd
                    )
                    # Update KB: move matching experiment from Executing ‚Üí Completed
                    _update_kb_on_task_complete(
                        decision.target,
                        f"score {eval_result.overall_score:.0f}/{target_score}, PASSED",
                        cwd,
                    )
                else:
                    console.print(f"[yellow]‚ö† Task {decision.target} needs improvement (score: {eval_result.overall_score:.0f}/{target_score})[/yellow]")
                    append_to_progress_log(
                        f"EXECUTE {decision.target} - NEEDS IMPROVEMENT "
                        f"(score: {eval_result.overall_score:.0f}/{target_score} ‚Äî BELOW TARGET, "
                        f"metrics_found: {len(eval_result.metrics)}, "
                        f"details in tasks/{decision.target}.md)",
                        cwd
                    )

                # Record state for auto-continue on next iteration (Exp 3)
                _exec_auto_state = {
                    "action": "execute",
                    "task_id": decision.target,
                    "verdict": "passed",
                    "score": eval_result.overall_score,
                    "should_pivot": eval_result.should_pivot,
                }

            else:
                console.print(f"[red]‚úó Execution failed: {result.error}[/red]")
                append_to_progress_log(
                    f"EXECUTE {decision.target} - ERROR: {result.error}",
                    cwd
                )

            # Update last_iter_state for auto-continue tracking (Exp 3)
            last_iter_state = _exec_auto_state

            logger.log_iteration_end(i, decision.action.value, success=result.success)
            continue

        # Handle MODIFY
        if decision.action == Action.MODIFY:
            if not decision.target:
                console.print("[red]Error: MODIFY requires a target task[/red]")
                logger.log_error("MODIFY requires a target task")
                continue

            console.print(f"\n[yellow]Modifying {decision.target}...[/yellow]")
            console.print(f"[dim]Modification: {decision.modification}[/dim]")

            # Read existing task content and append modification note
            task_content = read_task(decision.target, cwd)
            if task_content:
                now = datetime.now().strftime("%Y-%m-%d %H:%M")
                modification_note = f"\n\n## Modification ({now})\n{decision.modification}\n"
                write_task(decision.target, task_content + modification_note, cwd)
                console.print(f"[green]‚úì Task {decision.target} modified[/green]")
            else:
                console.print(f"[red]Error: Task {decision.target} not found[/red]")
                logger.log_error(f"Task {decision.target} not found for MODIFY")

            logger.log_task_modified(decision.target, decision.modification)
            append_to_progress_log(
                f"MODIFY {decision.target} - {decision.modification[:100]}",
                cwd
            )
            logger.log_iteration_end(i, decision.action.value)
            continue

        # Handle SKIP
        if decision.action == Action.SKIP:
            consecutive_skips += 1
            console.print(f"\n[yellow]‚è≠ Skipping {decision.target or 'current task'} (consecutive: {consecutive_skips}/3)[/yellow]")
            console.print(f"[dim]Reason: {decision.reason}[/dim]")

            logger.log_task_skipped(decision.target, decision.reason)
            append_to_progress_log(
                f"SKIP {decision.target or 'task'} - {decision.reason} (consecutive: {consecutive_skips})",
                cwd
            )

            if consecutive_skips >= 3:
                console.print("\n[bold yellow]‚ö† 3 consecutive skips detected ‚Äî auto-completing to avoid loop[/bold yellow]")
                append_to_progress_log(
                    f"AUTO_DONE - Exiting after {consecutive_skips} consecutive skips",
                    cwd
                )
                logger.log_iteration_end(i, "AUTO_DONE")
                _print_session_summary(logger, i)
                generate_handoff_note(cwd)  # Save state for resume
                logger.log_session_end(success=False, reason=f"Auto-done after {consecutive_skips} consecutive skips")
                return False

            logger.log_iteration_end(i, decision.action.value)
            continue

        # Handle ASK
        if decision.action == Action.ASK:
            console.print(f"\n[bold cyan]‚ùì Planner needs your input:[/bold cyan]")
            console.print(f"\n{decision.question}\n")

            # Get user input
            user_answer = Prompt.ask("[bold]Your answer[/bold]")

            # Log the Q&A
            logger.log_user_question(decision.question, user_answer)
            append_to_progress_log(
                f"ASK - Q: {decision.question[:100]}... A: {user_answer[:100]}",
                cwd
            )

            # Append Q&A to pool.md Findings section (atomic, lock-protected)
            qa_note = f"**User Decision**: {decision.question} ‚Üí {user_answer}"
            append_to_findings(qa_note, cwd)

            console.print(f"[green]‚úì Answer recorded[/green]")
            logger.log_iteration_end(i, decision.action.value)
            continue

        # Handle HEDGE / PIVOT_WAIT - Pessimistic Preparation
        if decision.action in (Action.HEDGE, Action.PIVOT_WAIT):
            console.print(f"\n[yellow]üõ°Ô∏è HEDGE: ‰∏∫ {decision.target} Êé¢Á¥¢Êõø‰ª£ÊñπÊ°à[/yellow]")
            console.print(f"[dim]Reason: {decision.reason}[/dim]")

            notify_pivot(
                task_id=decision.target,
                reason=decision.reason,
                from_approach="ÂΩìÂâçÂÆûÁé∞",
                to_approach="Êõø‰ª£ÊñπÊ°àÊé¢Á¥¢‰∏≠",
                trigger="wait",
            )

            if decision.failure_assumptions:
                append_failure_assumptions(decision.target, decision.failure_assumptions, cwd)
                console.print(f"[dim]ËÆ∞ÂΩï‰∫ÜÂ§±Ë¥•ÂÅáËÆæÂà∞ pool.md[/dim]")

            _create_tasks_from_decision(decision, logger, cwd, "EXPLORE")
            _handle_pivot_tail(
                decision, logger, i,
                f"HEDGE {decision.target} - Êé¢Á¥¢Êõø‰ª£ÊñπÊ°à: {decision.reason[:100]}",
                cwd,
            )
            continue

        # Handle PIVOT_RESEARCH - Research confirmed direction not viable
        if decision.action == Action.PIVOT_RESEARCH:
            console.print(f"\n[magenta]üîÑ PIVOT_RESEARCH: {decision.target} ÊñπÂêë‰∏çÂèØË°å[/magenta]")
            console.print(f"[dim]ÂΩìÂâçÊñπÊ°à: {decision.current_approach}[/dim]")
            console.print(f"[dim]ÈòªÊñ≠ÂéüÂõ†: {decision.blocker}[/dim]")
            console.print(f"[dim]Êñ∞ÊñπÂêë: {decision.new_direction}[/dim]")

            notify_pivot(
                task_id=decision.target,
                reason=decision.blocker,
                from_approach=decision.current_approach,
                to_approach=decision.new_direction,
                trigger="research",
            )
            notify_decision(
                decision=f"ÊîæÂºÉ {decision.current_approach}ÔºåËΩ¨Âêë {decision.new_direction}",
                reason=decision.blocker,
                task_id=decision.target,
            )

            _create_tasks_from_decision(decision, logger, cwd, "EXPLORE")
            _handle_pivot_tail(
                decision, logger, i,
                f"PIVOT_RESEARCH {decision.target} - ÊîæÂºÉ [{decision.current_approach}] Âõ†‰∏∫ [{decision.blocker}]ÔºåËΩ¨Âêë [{decision.new_direction}]",
                cwd,
            )
            continue

        # Handle PIVOT_ITERATION - Multiple attempts failed
        if decision.action == Action.PIVOT_ITERATION:
            console.print(f"\n[magenta]üîÑ PIVOT_ITERATION: {decision.target} Â§öÊ¨°Â∞ùËØïÊú™ËææÊ†á[/magenta]")
            console.print(f"[dim]Â∞ùËØïÊ¨°Êï∞: {decision.attempt_count}[/dim]")
            console.print(f"[dim]ÊúÄÈ´òÂàÜÊï∞: {decision.best_score}[/dim]")
            console.print(f"[dim]Â§±Ë¥•Ê®°Âºè: {decision.failure_pattern}[/dim]")
            console.print(f"[dim]Êñ∞ÊñπÊ°à: {decision.new_approach}[/dim]")

            notify_pivot(
                task_id=decision.target,
                reason=f"Â∞ùËØï {decision.attempt_count} Ê¨°ÔºåÊúÄÈ´òÂàÜ {decision.best_score}Ôºå{decision.failure_pattern}",
                from_approach=f"Áé∞ÊúâÂÆûÁé∞ (Â∞ùËØï {decision.attempt_count} Ê¨°)",
                to_approach=decision.new_approach,
                trigger="iteration",
            )
            notify_decision(
                decision=f"ÂÅúÊ≠¢‰ºòÂåñÁé∞ÊúâÂÆûÁé∞ÔºåÈááÁî®Êñ∞ÊñπÊ°à: {decision.new_approach}",
                reason=decision.failure_pattern,
                task_id=decision.target,
            )

            _create_tasks_from_decision(decision, logger, cwd, "IMPLEMENT")
            _handle_pivot_tail(
                decision, logger, i,
                f"PIVOT_ITERATION {decision.target} - Â∞ùËØï {decision.attempt_count} Ê¨°ÔºåÊúÄÈ´òÂàÜ {decision.best_score}ÔºåËΩ¨Âêë [{decision.new_approach}]",
                cwd,
            )
            continue

      except asyncio.CancelledError:
        # SDK async generator GC can cancel the main task via anyio cancel scopes.
        # async_generator_athrow = Python GC finalizer for unclosed generators.
        # This is benign ‚Äî actual work completed, only current await interrupted.
        #
        # Safe to catch unconditionally: Ctrl+C raises KeyboardInterrupt (not
        # CancelledError), and this codebase has no task.cancel() or wait_for()
        # calls, so CancelledError here is always from anyio cancel scope cleanup.
        console.print(f"[dim]‚ö† Recovered from SDK async generator cleanup (retrying iteration {i})[/dim]")
        logger.log_iteration_end(i, "RECOVERED", reason="SDK cancel scope cleanup")
        continue

    console.print(f"\n[yellow]Reached max iterations ({max_iterations})[/yellow]")
    _print_session_summary(logger, max_iterations)
    generate_handoff_note(cwd)  # Save state for resume
    logger.log_session_end(success=False, reason=f"Reached max iterations ({max_iterations})")
    return False


async def resume(
    cwd: str = ".",
    max_iterations: int = 30,
    verbose: bool = False,
    max_parallel: int = DEFAULT_MAX_PARALLEL,
    target_score: int = None,
    explore_mode: bool = False,
    thinking_budget: int | None = None,
    no_sandbox: bool = False,
    synthesis_interval: int = 3,
) -> bool:
    """
    Resume an existing task pool session.

    If the system is waiting for user feedback, it will process
    the feedback and incorporate it into the next iteration.

    Args:
        cwd: Working directory containing .ralph/
        max_iterations: Maximum number of iterations
        verbose: If True, show detailed tool calls and thinking
        max_parallel: Maximum number of concurrent workers for PARALLEL_EXECUTE
        target_score: Target quality score. If None, parsed from goal.md or defaults to 95.
        explore_mode: If True, don't DONE until user explicitly says stop.
        synthesis_interval: Run synthesis after every N new completed tasks.
            0 disables periodic synthesis. Default 3.

    Returns:
        True if goal was completed, False if max iterations reached
    """
    if not goal_exists(cwd):
        raise ValueError("No existing session found (goal.md missing)")
    if not pool_exists(cwd):
        raise ValueError("No existing session found (pool.md missing)")

    console.print("\n[bold]Resuming existing session[/bold]\n")

    # Read and display handoff note if available
    handoff = read_handoff_note(cwd)
    if handoff:
        console.print("[dim]üìã Found handoff note from previous session[/dim]")
        # Inject handoff into pool.md Findings so Planner sees it
        append_to_findings(f"**[HANDOFF]** Resume context from previous session available in .ralph/handoff.md", cwd)
        # Don't clear yet - let planner read it via build_planner_prompt

    # Check if we were waiting for user feedback
    if is_waiting_for_user(cwd):
        feedback = parse_feedback(cwd)
        manifest = read_checkpoint_manifest(cwd)

        if feedback and manifest:
            # Process feedback - append to pool.md findings
            console.print("[yellow]Processing user feedback...[/yellow]\n")

            feedback_summary = "**User Testing Feedback**:\n"

            # Find best checkpoint
            best_cp = None
            best_score = 0
            for cp_id, results in feedback.get("checkpoint_results", {}).items():
                score = int(results.get("ËØÑÂàÜ (1-5)", "0") or "0")
                feedback_summary += f"- {cp_id}: ËØÑÂàÜ {score}/5"
                if results.get("ÊàêÂäüÁéá"):
                    feedback_summary += f", ÊàêÂäüÁéá {results['ÊàêÂäüÁéá']}"
                feedback_summary += "\n"
                if score > best_score:
                    best_score = score
                    best_cp = cp_id

            if feedback.get("overall_feedback"):
                feedback_summary += f"\n**Overall**: {feedback['overall_feedback']}\n"

            if best_cp:
                feedback_summary += f"\n**Best checkpoint**: {best_cp} (score: {best_score}/5)\n"

            feedback_summary += f"\n**Next step**: {feedback.get('next_step', 'continue')}\n"

            # Append to pool findings
            pool_content = read_pool(cwd)
            if "## Findings" in pool_content:
                pool_content = pool_content.replace(
                    "## Findings",
                    f"## Findings\n\n{feedback_summary}"
                )
                write_pool(pool_content, cwd)

            # Update manifest status
            save_checkpoint_manifest(
                checkpoints=manifest.get("checkpoints", []),
                status="feedback_processed",
                instructions=manifest.get("instructions", ""),
                cwd=cwd,
            )

            append_to_progress_log(
                f"Processed user feedback. Best checkpoint: {best_cp}. Next: {feedback.get('next_step', 'continue')}",
                cwd
            )

            # Handle next step
            if feedback.get("next_step") == "finish":
                console.print("[green]‚úì User indicated task is complete[/green]")
                return True

    # Just run the iteration loop (goal already exists)
    return await run(
        goal="",  # Not used since goal.md exists
        cwd=cwd,
        max_iterations=max_iterations,
        skip_clarify=True,
        verbose=verbose,
        max_parallel=max_parallel,
        target_score=target_score,
        explore_mode=explore_mode,
        thinking_budget=thinking_budget,
        no_sandbox=no_sandbox,
        synthesis_interval=synthesis_interval,
    )
