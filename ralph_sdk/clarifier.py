"""
Requirements clarification module.

Clarifies user requirements through interactive Q&A and outputs to goal.md.
Includes dynamic metrics clarification with AI-generated questions.
"""

import re
from typing import Optional

import questionary
from questionary import Style
from claude_code_sdk import AssistantMessage, ClaudeCodeOptions, query
from rich.console import Console
from rich.panel import Panel
from rich.table import Table

from .metrics import (
    AutomationLevel,
    EvalConfig,
    MetricDefinition,
    MetricsConfig,
    MetricType,
    TaskCategory,
    detect_category,
    get_default_metrics,
)
from .pool import init_ralph_dir, write_goal
from .prompts import CLARIFIER_SYSTEM_PROMPT, CLARIFIER_V2_SYSTEM_PROMPT, CLARIFIER_V2_EXPLORE_PROMPT

console = Console()

# Custom style for questionary
custom_style = Style([
    ('qmark', 'fg:cyan bold'),
    ('question', 'fg:white bold'),
    ('answer', 'fg:cyan'),
    ('pointer', 'fg:cyan bold'),
    ('highlighted', 'fg:cyan bold'),
    ('selected', 'fg:green'),
])


# =============================================================================
# Base Questions (always asked)
# =============================================================================

Q_PURPOSE = {
    "question": "è¿™ä¸ªé¡¹ç›®çš„ä¸»è¦ç”¨é€”æ˜¯ä»€ä¹ˆï¼Ÿ",
    "options": [
        "ç”Ÿäº§éƒ¨ç½² (ç»™çœŸå®ç”¨æˆ·ç”¨)",
        "ç ”ç©¶å®éªŒ (å‘è®ºæ–‡ã€éªŒè¯æƒ³æ³•)",
        "å­¦ä¹ æ¢ç´¢ (å­¦ä¹ æ–°æŠ€æœ¯)",
        "åŸå‹éªŒè¯ (å¿«é€ŸéªŒè¯å¯è¡Œæ€§)",
    ],
}

Q_EVAL_MODE = {
    "question": "è¯„ä¼°/æµ‹è¯•ä¸»è¦ç”±è°æ¥åšï¼Ÿ",
    "options": [
        "å…¨è‡ªåŠ¨ (æœ‰ç°æˆ benchmark/æµ‹è¯•é›†)",
        "åŠè‡ªåŠ¨ (æœ‰ä»£ç†æŒ‡æ ‡ï¼Œä½†æœ€ç»ˆéœ€äººå·¥ç¡®è®¤)",
        "äººå·¥ä¸ºä¸» (éœ€è¦åœ¨çœŸå®ç¯å¢ƒæµ‹è¯•)",
    ],
}

Q_TEST_FREQUENCY = {
    "question": "ä½ å¤§æ¦‚å¤šä¹…èƒ½æµ‹è¯•ä¸€æ¬¡ï¼Ÿ",
    "options": [
        "å®æ—¶ (æˆ‘ä¼šä¸€ç›´ç›¯ç€ï¼Œéšæ—¶å¯ä»¥æµ‹)",
        "æ¯å°æ—¶ (æˆ‘ä¼šå®šæœŸæ¥çœ‹)",
        "æ¯å¤© (æ™šä¸Š/ç¬¬äºŒå¤©æ¥çœ‹ç»“æœ)",
        "æ›´ä¹… (éœ€è¦å®‰æ’ä¸“é—¨æ—¶é—´æµ‹è¯•)",
    ],
}

Q_BATCH_PREFERENCE = {
    "question": "å¸Œæœ›æ€ä¹ˆå®‰æ’æµ‹è¯•ï¼Ÿ",
    "options": [
        "ä¸€ä¸ªä¸€ä¸ªæµ‹ (Agent å‡ºä¸€ä¸ªæ–¹æ¡ˆï¼Œæˆ‘æµ‹å®Œå†ç»§ç»­)",
        "æ‰¹é‡æµ‹ (Agent å…ˆå‡ºå¤šä¸ªæ–¹æ¡ˆï¼Œæˆ‘ä¸€èµ·æµ‹)",
        "è‡ªåŠ¨ç­›é€‰ (Agent ç”¨ä»£ç†æŒ‡æ ‡ç­›é€‰ï¼Œåªè®©æˆ‘æµ‹æœ€æœ‰å¸Œæœ›çš„)",
    ],
}


# =============================================================================
# AI Prompts for Dynamic Generation
# =============================================================================

DYNAMIC_QUESTION_PROMPT = """
ç”¨æˆ·æ­£åœ¨æè¿°ä»–ä»¬æƒ³è¦æ„å»ºçš„é¡¹ç›®ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„æè¿°ï¼Œç”Ÿæˆ 2-3 ä¸ªé’ˆå¯¹æ€§çš„é€‰æ‹©é¢˜ï¼Œå¸®åŠ©æ¾„æ¸…é¡¹ç›®çš„å…³é”®çº¦æŸå’Œè¯„ä¼°æŒ‡æ ‡ã€‚

## ç”¨æˆ·æè¿°
{goal}

## ç”¨é€”
{purpose}

## è¦æ±‚
1. æ¯ä¸ªé—®é¢˜å¿…é¡»æ˜¯é€‰æ‹©é¢˜ï¼Œæœ‰ 3-4 ä¸ªé€‰é¡¹
2. é—®é¢˜è¦é’ˆå¯¹è¿™ä¸ªå…·ä½“åœºæ™¯ï¼Œä¸è¦å¤ªé€šç”¨
3. å…³æ³¨å¯¹è¯„ä¼°æŒ‡æ ‡æœ‰å½±å“çš„å› ç´ ï¼ˆå»¶è¿Ÿã€å‡†ç¡®æ€§ã€æˆæœ¬ç­‰ï¼‰
4. é€‰é¡¹è¦å…·ä½“ï¼Œæœ€å¥½æœ‰æ•°å­—èŒƒå›´
5. ä¸è¦é—®"ä½ æ‹…å¿ƒä»€ä¹ˆé—®é¢˜"è¿™ç§æ³›æ³›çš„é—®é¢˜ï¼Œè¦é—®å…·ä½“çš„æŠ€æœ¯çº¦æŸ

## è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼æŒ‰æ­¤æ ¼å¼ï¼‰

QUESTION: <é—®é¢˜æ–‡å­—>
A: <é€‰é¡¹A>
B: <é€‰é¡¹B>
C: <é€‰é¡¹C>
D: <é€‰é¡¹Dï¼ˆå¯é€‰ï¼‰>

QUESTION: <ä¸‹ä¸€ä¸ªé—®é¢˜>
...

åªè¾“å‡ºé—®é¢˜ï¼Œä¸è¦æœ‰å…¶ä»–è§£é‡Šã€‚
"""

METRIC_GENERATION_PROMPT = """
æ ¹æ®ç”¨æˆ·çš„é¡¹ç›®æè¿°å’Œå›ç­”ï¼Œç”Ÿæˆå…·ä½“çš„è¯„ä¼°æŒ‡æ ‡ã€‚

## ç”¨æˆ·æè¿°
{goal}

## ç”¨é€”
{purpose}

## ç”¨æˆ·å›ç­”
{answers}

## è¯„ä¼°æ¨¡å¼
{eval_mode}

## è¦æ±‚
ç”Ÿæˆ 2-4 ä¸ªå…·ä½“çš„è¯„ä¼°æŒ‡æ ‡ï¼Œæ¯ä¸ªæŒ‡æ ‡åŒ…æ‹¬ï¼š
- åç§°ï¼ˆè‹±æ–‡ï¼Œsnake_caseï¼Œå¦‚ response_latencyï¼‰
- ç±»å‹ï¼ˆhard = å¿…é¡»è¾¾åˆ° / soft = ç›®æ ‡å€¼ / subjective = ä¸»è§‚è¯„ä¼°ï¼‰
- ç›®æ ‡å€¼ï¼ˆå…·ä½“æ•°å­—ï¼Œå¦‚ <= 50ms, >= 90%ï¼‰
- ä¸ºä»€ä¹ˆé‡è¦ï¼ˆä¸€å¥è¯ï¼Œé’ˆå¯¹è¿™ä¸ªå…·ä½“é¡¹ç›®ï¼‰
- å¦‚ä½•æµ‹é‡ï¼ˆå…·ä½“æ–¹æ³•ï¼Œè¦å¯æ‰§è¡Œï¼‰
- è‡ªåŠ¨åŒ–ç¨‹åº¦ï¼ˆauto = å¯è‡ªåŠ¨æµ‹è¯• / manual = éœ€è¦äººå·¥æµ‹è¯• / hybrid = å¯ç”¨ä»£ç†æŒ‡æ ‡è‡ªåŠ¨æµ‹ï¼Œæœ€ç»ˆéœ€äººå·¥ç¡®è®¤ï¼‰

å¦‚æœæ˜¯ manual æˆ– hybrid ç±»å‹çš„æŒ‡æ ‡ï¼Œè¿˜éœ€è¦æä¾›ï¼š
- ä»£ç†æŒ‡æ ‡ï¼ˆproxy_metricï¼‰ï¼šä¸€ä¸ªå¯ä»¥è‡ªåŠ¨æµ‹è¯•çš„è¿‘ä¼¼æŒ‡æ ‡
- æ‰¹é‡æµ‹è¯•å»ºè®®ï¼ˆbatch_suggestionï¼‰ï¼šå¦‚ä½•è®©ç”¨æˆ·é«˜æ•ˆæ‰¹é‡æµ‹è¯•

## è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼æŒ‰æ­¤æ ¼å¼ï¼‰

METRIC: <è‹±æ–‡åç§°>
TYPE: <hard|soft|subjective>
TARGET: <ç›®æ ‡å€¼>
WHY: <ä¸ºä»€ä¹ˆé‡è¦>
MEASURE: <å¦‚ä½•æµ‹é‡>
AUTOMATION: <auto|manual|hybrid>
PROXY: <ä»£ç†æŒ‡æ ‡ï¼Œå¦‚æœ AUTOMATION ä¸æ˜¯ auto>
BATCH: <æ‰¹é‡æµ‹è¯•å»ºè®®ï¼Œå¦‚æœ AUTOMATION ä¸æ˜¯ auto>

METRIC: <ä¸‹ä¸€ä¸ªæŒ‡æ ‡>
...

åªè¾“å‡ºæŒ‡æ ‡ï¼Œä¸è¦æœ‰å…¶ä»–è§£é‡Šã€‚
"""


# =============================================================================
# Parsing Functions
# =============================================================================

def parse_dynamic_questions(text: str) -> list[dict]:
    """Parse AI-generated questions from text."""
    questions = []
    blocks = re.split(r'\n?QUESTION:\s*', text)

    for block in blocks[1:]:
        lines = block.strip().split('\n')
        if not lines:
            continue

        question = {"question": lines[0].strip(), "options": []}

        for line in lines[1:]:
            line = line.strip()
            match = re.match(r'^([A-D])[\.:]\s*(.+)$', line)
            if match:
                value = match.group(2).strip()
                if value:
                    question["options"].append(value)

        if question["question"] and len(question["options"]) >= 2:
            questions.append(question)

    return questions


def parse_metrics(text: str) -> list[dict]:
    """Parse AI-generated metrics from text."""
    metrics = []
    blocks = re.split(r'\n?METRIC:\s*', text)

    for block in blocks[1:]:
        metric = {}

        name_match = re.match(r'^(\S+)', block)
        if name_match:
            metric['name'] = name_match.group(1).strip()

        type_match = re.search(r'TYPE:\s*(\w+)', block, re.IGNORECASE)
        if type_match:
            metric['type'] = type_match.group(1).lower()

        target_match = re.search(r'TARGET:\s*(.+?)(?=\n|$)', block, re.IGNORECASE)
        if target_match:
            metric['target'] = target_match.group(1).strip()

        why_match = re.search(r'WHY:\s*(.+?)(?=\n|$)', block, re.IGNORECASE)
        if why_match:
            metric['why'] = why_match.group(1).strip()

        measure_match = re.search(r'MEASURE:\s*(.+?)(?=\n|$)', block, re.IGNORECASE)
        if measure_match:
            metric['how_to_measure'] = measure_match.group(1).strip()

        auto_match = re.search(r'AUTOMATION:\s*(\w+)', block, re.IGNORECASE)
        if auto_match:
            metric['automation'] = auto_match.group(1).lower()

        proxy_match = re.search(r'PROXY:\s*(.+?)(?=\n|$)', block, re.IGNORECASE)
        if proxy_match:
            metric['proxy_metric'] = proxy_match.group(1).strip()

        batch_match = re.search(r'BATCH:\s*(.+?)(?=\n|$)', block, re.IGNORECASE)
        if batch_match:
            metric['batch_suggestion'] = batch_match.group(1).strip()

        if metric.get('name'):
            metrics.append(metric)

    return metrics


# =============================================================================
# Interactive Q&A with questionary
# =============================================================================

async def ask_select(question: str, options: list[str], allow_custom: bool = True) -> str:
    """Ask a selection question with optional custom input."""
    if allow_custom:
        choices = options + ["[è‡ªå·±è¾“å…¥]"]
    else:
        choices = options

    answer = await questionary.select(
        question,
        choices=choices,
        style=custom_style,
        use_shortcuts=False,
        use_indicator=True,
    ).ask_async()

    if answer == "[è‡ªå·±è¾“å…¥]":
        answer = await questionary.text(
            "è¯·è¾“å…¥ä½ çš„å›ç­”:",
            style=custom_style,
        ).ask_async()

    return answer or ""


# =============================================================================
# AI Generation Functions
# =============================================================================

async def generate_dynamic_questions(goal: str, purpose: str) -> list[dict]:
    """Use AI to generate context-specific questions."""
    console.print("\n[dim]åˆ†æéœ€æ±‚ï¼Œç”Ÿæˆé’ˆå¯¹æ€§é—®é¢˜...[/dim]")

    prompt = DYNAMIC_QUESTION_PROMPT.format(goal=goal, purpose=purpose)

    result_text = ""
    async for message in query(
        prompt=prompt,
        options=ClaudeCodeOptions(
            system_prompt="ä½ æ˜¯ä¸€ä¸ªå¸®åŠ©æ¾„æ¸…é¡¹ç›®éœ€æ±‚çš„åŠ©æ‰‹ã€‚åªè¾“å‡ºè¦æ±‚çš„æ ¼å¼ï¼Œä¸è¦æœ‰å¤šä½™è§£é‡Šã€‚",
            allowed_tools=[],
            max_turns=1,
        ),
    ):
        if isinstance(message, AssistantMessage):
            for block in message.content:
                if hasattr(block, "text"):
                    result_text += block.text

    return parse_dynamic_questions(result_text)


async def generate_metrics(
    goal: str,
    purpose: str,
    answers: dict,
    eval_mode: str,
) -> list[dict]:
    """Use AI to generate metrics based on answers."""
    console.print("\n[dim]æ ¹æ®å›ç­”ç”Ÿæˆè¯„ä¼°æŒ‡æ ‡...[/dim]")

    answers_text = "\n".join([f"- {k}: {v}" for k, v in answers.items()])

    prompt = METRIC_GENERATION_PROMPT.format(
        goal=goal,
        purpose=purpose,
        answers=answers_text,
        eval_mode=eval_mode,
    )

    result_text = ""
    async for message in query(
        prompt=prompt,
        options=ClaudeCodeOptions(
            system_prompt="ä½ æ˜¯ä¸€ä¸ªå¸®åŠ©å®šä¹‰è¯„ä¼°æŒ‡æ ‡çš„åŠ©æ‰‹ã€‚åªè¾“å‡ºè¦æ±‚çš„æ ¼å¼ï¼Œä¸è¦æœ‰å¤šä½™è§£é‡Šã€‚",
            allowed_tools=[],
            max_turns=1,
        ),
    ):
        if isinstance(message, AssistantMessage):
            for block in message.content:
                if hasattr(block, "text"):
                    result_text += block.text

    return parse_metrics(result_text)


# =============================================================================
# Base Metrics (always included)
# =============================================================================

BASE_METRICS = [
    {
        "name": "runs_without_error",
        "type": "hard",
        "target": "pass",
        "why": "åŸºç¡€è¦æ±‚ï¼šä»£ç èƒ½æ­£å¸¸è¿è¡Œ",
        "automation": "auto",
    },
    {
        "name": "basic_functionality",
        "type": "hard",
        "target": "pass",
        "why": "åŸºç¡€è¦æ±‚ï¼šæ ¸å¿ƒåŠŸèƒ½æ­£å¸¸",
        "automation": "auto",
    },
]


# =============================================================================
# Main Clarification Functions
# =============================================================================

async def clarify_metrics(goal_description: str) -> tuple[MetricsConfig, EvalConfig, dict]:
    """
    Clarify success metrics through dynamic Q&A.

    Args:
        goal_description: The goal description

    Returns:
        Tuple of (MetricsConfig, EvalConfig, all_answers)
    """
    all_answers = {"goal": goal_description}

    console.print(f"\n[bold cyan]Success Metrics Configuration[/bold cyan]\n")

    # Q1: Purpose
    purpose = await ask_select(Q_PURPOSE["question"], Q_PURPOSE["options"], allow_custom=False)
    all_answers["purpose"] = purpose

    # Q2: Evaluation mode
    console.print()
    eval_mode = await ask_select(Q_EVAL_MODE["question"], Q_EVAL_MODE["options"], allow_custom=False)
    all_answers["eval_mode"] = eval_mode

    # Build EvalConfig
    eval_config = EvalConfig(mode=eval_mode)

    # If manual/hybrid, ask follow-up questions
    if "äººå·¥" in eval_mode or "åŠè‡ªåŠ¨" in eval_mode:
        console.print()
        test_freq = await ask_select(Q_TEST_FREQUENCY["question"], Q_TEST_FREQUENCY["options"], allow_custom=True)
        all_answers["test_frequency"] = test_freq
        eval_config.test_frequency = test_freq

        console.print()
        batch_pref = await ask_select(Q_BATCH_PREFERENCE["question"], Q_BATCH_PREFERENCE["options"], allow_custom=True)
        all_answers["batch_preference"] = batch_pref
        eval_config.batch_preference = batch_pref

    # Generate dynamic questions
    dynamic_questions = await generate_dynamic_questions(goal_description, purpose)

    if dynamic_questions:
        for q in dynamic_questions:
            console.print()
            answer = await ask_select(q["question"], q["options"], allow_custom=True)
            all_answers[q["question"]] = answer

    # Generate metrics
    dynamic_metrics = await generate_metrics(goal_description, purpose, all_answers, eval_mode)

    # Combine base + dynamic metrics
    all_metrics = BASE_METRICS + dynamic_metrics

    # Display metrics
    console.print("\n[bold green]ç”Ÿæˆçš„è¯„ä¼°æŒ‡æ ‡[/bold green]\n")

    table = Table(show_header=True)
    table.add_column("æŒ‡æ ‡", style="cyan", width=25)
    table.add_column("ç±»å‹", style="dim", width=10)
    table.add_column("ç›®æ ‡", style="yellow", width=15)
    table.add_column("è‡ªåŠ¨åŒ–", width=10)
    table.add_column("ä¸ºä»€ä¹ˆé‡è¦", width=35)

    for m in all_metrics:
        auto_display = {
            "auto": "[green]è‡ªåŠ¨[/green]",
            "manual": "[yellow]äººå·¥[/yellow]",
            "hybrid": "[cyan]æ··åˆ[/cyan]",
        }.get(m.get("automation", "auto"), "è‡ªåŠ¨")

        table.add_row(
            m.get('name', ''),
            m.get('type', ''),
            m.get('target', ''),
            auto_display,
            m.get('why', '')[:35],
        )

    console.print(table)

    # Confirm
    console.print()
    confirm = await ask_select(
        "è¿™äº›æŒ‡æ ‡å¯ä»¥å—ï¼Ÿ",
        ["å¯ä»¥ï¼Œå°±è¿™æ ·", "éœ€è¦è°ƒæ•´"],
        allow_custom=True,
    )

    if confirm != "å¯ä»¥ï¼Œå°±è¿™æ ·":
        console.print(f"[dim]è®°å½•è°ƒæ•´æ„è§: {confirm}[/dim]")
        all_answers["adjustment"] = confirm

    # Convert to MetricsConfig
    category = detect_category(goal_description)
    metrics_config = MetricsConfig(category=category, eval_config=eval_config)

    for m in all_metrics:
        metric_type = MetricType(m.get("type", "soft"))
        automation = AutomationLevel(m.get("automation", "auto"))

        metric_def = MetricDefinition(
            name=m["name"],
            type=metric_type,
            description=m.get("why", ""),
            target=m.get("target"),
            automation=automation,
            proxy_metric=m.get("proxy_metric"),
            batch_suggestion=m.get("batch_suggestion"),
        )

        if metric_type == MetricType.HARD:
            metrics_config.hard_constraints.append(metric_def)
        elif metric_type == MetricType.SOFT:
            metrics_config.soft_targets.append(metric_def)
        else:
            metrics_config.subjective_criteria.append(metric_def)

    console.print("\n[green]âœ“ Metrics configured[/green]")
    return metrics_config, eval_config, all_answers


def generate_goal_md(
    initial_prompt: str,
    summary_text: str,
    metrics_config: MetricsConfig,
    eval_config: EvalConfig,
    qa_text: str = "",
    answers_text: str = "",
) -> str:
    """Generate complete goal.md content."""
    lines = ["# Goal", ""]

    # Original request
    lines.append("## Original Request")
    lines.append(initial_prompt)
    lines.append("")

    # Q&A section (if any)
    if qa_text and answers_text:
        lines.append("## Clarification")
        lines.append("")
        lines.append("### Questions")
        lines.append(qa_text)
        lines.append("")
        lines.append("### Answers")
        lines.append(answers_text)
        lines.append("")

    # Clarified description
    lines.append("## Clarified Description")
    lines.append(summary_text)
    lines.append("")

    # Evaluation Mode section
    if eval_config.needs_user_testing():
        lines.append("## Evaluation Mode")
        lines.append("")
        lines.append(f"- **æµ‹è¯•æ¨¡å¼**: {eval_config.mode}")
        if eval_config.test_frequency:
            lines.append(f"- **æµ‹è¯•é¢‘ç‡**: {eval_config.test_frequency}")
        if eval_config.batch_preference:
            lines.append(f"- **æµ‹è¯•å®‰æ’**: {eval_config.batch_preference}")
        lines.append("")

    # Success Metrics
    lines.append("## Success Metrics")
    lines.append("")

    # Hard constraints
    if metrics_config.hard_constraints:
        lines.append("### Hard Constraints (must pass)")
        for m in metrics_config.hard_constraints:
            auto_tag = "[auto]" if m.automation == AutomationLevel.AUTO else "[manual]"
            lines.append(f"- [ ] **{m.name}** {auto_tag}: {m.target or 'pass'} - {m.description}")
        lines.append("")

    # Soft targets
    if metrics_config.soft_targets:
        lines.append("### Performance Targets")
        lines.append("| Metric | Target | Automation | Proxy |")
        lines.append("|--------|--------|------------|-------|")
        for m in metrics_config.soft_targets:
            proxy = m.proxy_metric or "-"
            lines.append(f"| {m.name} | {m.target or 'N/A'} | {m.automation.value} | {proxy} |")
        lines.append("")

    # Subjective criteria
    if metrics_config.subjective_criteria:
        lines.append("### Quality Criteria (AI-evaluated)")
        for m in metrics_config.subjective_criteria:
            lines.append(f"- **{m.name}**: {m.description}")
        lines.append("")

    # Manual testing instructions
    manual_metrics = [
        m for m in metrics_config.all_metrics()
        if m.automation in (AutomationLevel.MANUAL, AutomationLevel.HYBRID)
    ]
    if manual_metrics:
        lines.append("### Manual Testing Instructions")
        for m in manual_metrics:
            if m.batch_suggestion:
                lines.append(f"- **{m.name}**: {m.batch_suggestion}")
        lines.append("")

    return "\n".join(lines)


async def clarify_requirements(initial_prompt: str, cwd: str = ".") -> str:
    """
    Clarify requirements through interactive Q&A with the user.

    Args:
        initial_prompt: The user's initial feature request
        cwd: Working directory

    Returns:
        The clarified goal content (also written to goal.md)
    """
    init_ralph_dir(cwd)

    console.print(Panel(f"[bold]Feature Request:[/bold]\n{initial_prompt}", title="Input"))

    # Phase 1: Generate clarifying questions about functionality
    console.print("\n[yellow]Analyzing requirements...[/yellow]\n")

    questions_prompt = f"""User's feature request:
{initial_prompt}

First, explore the codebase to understand:
1. Project structure and tech stack
2. Existing patterns and conventions
3. Related existing functionality

Then generate 3-5 clarifying questions with lettered options to better understand the requirements.
Focus on scope, target users, core functionality.
"""

    questions_text = ""
    async for message in query(
        prompt=questions_prompt,
        options=ClaudeCodeOptions(
            system_prompt=CLARIFIER_SYSTEM_PROMPT,
            allowed_tools=[
                "Read", "Glob", "Grep", "LSP",
                "WebFetch", "WebSearch",
            ],
            max_turns=8,
            cwd=cwd,
        ),
    ):
        if isinstance(message, AssistantMessage):
            for block in message.content:
                if hasattr(block, "text"):
                    questions_text += block.text

    # Display questions and get answers
    console.print(Panel(questions_text, title="Clarifying Questions"))

    answers = await questionary.text(
        "Your answers (e.g., 1A, 2B, 3C or detailed response):",
        style=custom_style,
    ).ask_async() or ""

    # Phase 2: Generate summary
    console.print("\n[yellow]Generating clarified requirements...[/yellow]\n")

    summary_prompt = f"""Original request:
{initial_prompt}

Questions asked:
{questions_text}

User's answers:
{answers}

Based on this, provide a clear goal document with:
1. A clear, detailed description of what needs to be built
2. The scope (what's included)
3. Non-goals (what's explicitly NOT included)
4. Any important context from the codebase exploration

Format as markdown.
"""

    summary_text = ""
    async for message in query(
        prompt=summary_prompt,
        options=ClaudeCodeOptions(
            system_prompt=CLARIFIER_SYSTEM_PROMPT,
            allowed_tools=["Read", "Glob", "Grep", "LSP", "WebFetch", "WebSearch"],
            max_turns=5,
            cwd=cwd,
        ),
    ):
        if isinstance(message, AssistantMessage):
            for block in message.content:
                if hasattr(block, "text"):
                    summary_text += block.text

    console.print(Panel(summary_text, title="Clarified Requirements"))

    # Confirm
    confirm = await ask_select(
        "Proceed with these requirements?",
        ["Yes", "No", "Edit"],
        allow_custom=False,
    )

    if confirm == "No":
        raise KeyboardInterrupt("User cancelled")
    elif confirm == "Edit":
        edited = await questionary.text("Enter your revised requirements:", style=custom_style).ask_async()
        if edited:
            summary_text = edited

    # Phase 3: Clarify success metrics (dynamic)
    metrics_config, eval_config, _ = await clarify_metrics(summary_text)

    # Build goal.md content
    goal_content = generate_goal_md(
        initial_prompt=initial_prompt,
        summary_text=summary_text,
        metrics_config=metrics_config,
        eval_config=eval_config,
        qa_text=questions_text,
        answers_text=answers,
    )

    # Write to goal.md
    write_goal(goal_content, cwd)
    console.print("\n[green]âœ“ Goal saved to .ralph/goal.md[/green]")

    return goal_content


async def quick_clarify(initial_prompt: str, cwd: str = ".") -> str:
    """
    Quick clarification without interactive Q&A.
    Useful for simple, well-defined requests.

    Args:
        initial_prompt: The user's initial feature request
        cwd: Working directory

    Returns:
        The goal content (also written to goal.md)
    """
    init_ralph_dir(cwd)

    # Use default metrics based on detected category
    category = detect_category(initial_prompt)
    metrics_config = get_default_metrics(category)
    eval_config = EvalConfig(mode="å…¨è‡ªåŠ¨")

    goal_content = generate_goal_md(
        initial_prompt=initial_prompt,
        summary_text=initial_prompt,
        metrics_config=metrics_config,
        eval_config=eval_config,
    )

    write_goal(goal_content, cwd)
    console.print(f"\n[green]âœ“ Goal saved to .ralph/goal.md[/green]")
    console.print(f"[dim]Task category: {category.value}, using default metrics[/dim]")

    return goal_content


# =============================================================================
# Clarifier v2: Explore and Propose Mode
# =============================================================================

PROPOSAL_PARSE_PROMPT = """
ä»ä»¥ä¸‹æ¢ç´¢ç»“æœä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ã€‚

æ¢ç´¢ç»“æœï¼š
---
{exploration_result}
---

è¯·æå–å¹¶è¾“å‡ºä»¥ä¸‹ JSON æ ¼å¼ï¼š
```json
{
  "understanding": "å¯¹ç”¨æˆ·éœ€æ±‚çš„ä¸€å¥è¯ç†è§£",
  "proposals": [
    {
      "name": "æ–¹æ¡ˆåç§°",
      "summary": "ä¸€å¥è¯æ¦‚è¿°",
      "pros": ["ä¼˜ç‚¹1", "ä¼˜ç‚¹2"],
      "cons": ["ç¼ºç‚¹1", "ç¼ºç‚¹2"],
      "complexity": "ä½|ä¸­|é«˜",
      "risk": "ä¸»è¦é£é™©"
    }
  ],
  "recommendation": {
    "name": "æ¨èçš„æ–¹æ¡ˆåç§°",
    "reasons": ["åŸå› 1", "åŸå› 2", "åŸå› 3"]
  },
  "temporal_topics": ["éœ€è¦éªŒè¯çš„æ—¶æ•ˆæ€§è¯é¢˜1", "è¯é¢˜2"]
}
```

åªè¾“å‡º JSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚
"""


async def explore_and_propose(initial_prompt: str, cwd: str = ".") -> str:
    """
    Clarifier v2: Explore possible approaches and propose options to user.

    This is the "explore + propose" mode that:
    1. Researches this direction to discover possibilities
    2. Deeply analyzes each approach's pros/cons
    3. Makes recommendations for user to choose from

    Args:
        initial_prompt: The user's initial (possibly vague) request
        cwd: Working directory

    Returns:
        The clarified goal content (also written to goal.md)
    """
    init_ralph_dir(cwd)

    console.print(Panel(
        f"[bold]ç”¨æˆ·éœ€æ±‚:[/bold]\n{initial_prompt}",
        title="[cyan]Clarifier v2: æ¢ç´¢+æè®®æ¨¡å¼[/cyan]",
        border_style="cyan",
    ))

    # Phase 1: Deep exploration with AI
    console.print("\n[yellow]ğŸ” Phase 1: æ·±åº¦æ¢ç´¢ä¸­...[/yellow]")
    console.print("[dim]Agent æ­£åœ¨ç ”ç©¶å¯èƒ½çš„å®ç°æ–¹æ¡ˆï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...[/dim]\n")

    explore_prompt = CLARIFIER_V2_EXPLORE_PROMPT.format(user_request=initial_prompt)

    exploration_result = ""
    async for message in query(
        prompt=explore_prompt,
        options=ClaudeCodeOptions(
            system_prompt=CLARIFIER_V2_SYSTEM_PROMPT,
            allowed_tools=[
                "Read", "Glob", "Grep", "LSP",
                "WebFetch", "WebSearch", "Task",
            ],
            max_turns=20,  # Allow more turns for deep exploration
            cwd=cwd,
        ),
    ):
        if isinstance(message, AssistantMessage):
            for block in message.content:
                if hasattr(block, "text"):
                    exploration_result += block.text

    # Display the exploration result
    console.print("\n[bold cyan]â•â•â• æ¢ç´¢å®Œæˆ â•â•â•[/bold cyan]\n")
    console.print(Panel(exploration_result, title="æ–¹æ¡ˆåˆ†æ", border_style="cyan"))

    # Phase 2: User selection
    console.print("\n[bold yellow]è¯·é€‰æ‹©ä¸€ä¸ªæ–¹æ¡ˆ:[/bold yellow]\n")

    # Parse proposals from the exploration result to create options
    # Look for "æ–¹æ¡ˆ A:", "æ–¹æ¡ˆ B:", etc.
    proposal_pattern = r"###\s*æ–¹æ¡ˆ\s*([A-Z]):\s*([^\n]+)"
    proposals = re.findall(proposal_pattern, exploration_result)

    if proposals:
        options = [f"æ–¹æ¡ˆ {letter}: {name.strip()}" for letter, name in proposals]
        options.append("å…¶ä»–æƒ³æ³• (è‡ªå·±è¾“å…¥)")

        selection = await ask_select(
            "é€‰æ‹©ä½ æƒ³è¦çš„æ–¹æ¡ˆ:",
            options,
            allow_custom=False,
        )

        if "å…¶ä»–æƒ³æ³•" in selection:
            selection = await questionary.text(
                "è¯·æè¿°ä½ çš„æƒ³æ³•:",
                style=custom_style,
            ).ask_async() or ""
    else:
        # Fallback if parsing failed
        selection = await questionary.text(
            "é€‰æ‹©ä¸€ä¸ªæ–¹æ¡ˆ (A/B/C) æˆ–è¾“å…¥å…¶ä»–æƒ³æ³•:",
            style=custom_style,
        ).ask_async() or "A"

    console.print(f"\n[green]âœ“ é€‰æ‹©äº†: {selection}[/green]")

    # Phase 3: Generate clarified goal based on selection
    console.print("\n[yellow]ğŸ“ Phase 2: ç”Ÿæˆæ˜ç¡®ç›®æ ‡...[/yellow]\n")

    goal_generation_prompt = f"""ç”¨æˆ·çš„åŸå§‹éœ€æ±‚ï¼š
{initial_prompt}

æ¢ç´¢åˆ†æç»“æœï¼š
{exploration_result}

ç”¨æˆ·é€‰æ‹©ï¼š
{selection}

è¯·æ ¹æ®ç”¨æˆ·çš„é€‰æ‹©ï¼Œç”Ÿæˆä¸€ä¸ªæ˜ç¡®ã€å¯æ‰§è¡Œçš„ç›®æ ‡æè¿°ã€‚æ ¼å¼è¦æ±‚ï¼š

1. **Clarified Description** - åŸºäºé€‰æ‹©çš„æ–¹æ¡ˆï¼Œè¯¦ç»†æè¿°è¦åšä»€ä¹ˆ
2. **Scope** - åŒ…å«å“ªäº›åŠŸèƒ½
3. **Non-goals** - æ˜ç¡®ä¸åŒ…å«ä»€ä¹ˆ
4. **Technical Approach** - é€‰å®šæ–¹æ¡ˆçš„æŠ€æœ¯ç»†èŠ‚
5. **Risks and Mitigations** - ä¸»è¦é£é™©å’Œåº”å¯¹ç­–ç•¥

ä½¿ç”¨ Markdown æ ¼å¼è¾“å‡ºã€‚
"""

    summary_text = ""
    async for message in query(
        prompt=goal_generation_prompt,
        options=ClaudeCodeOptions(
            system_prompt="ä½ æ˜¯ä¸€ä¸ªå¸®åŠ©ç”Ÿæˆé¡¹ç›®ç›®æ ‡æ–‡æ¡£çš„åŠ©æ‰‹ã€‚è¯·åŸºäºç”¨æˆ·çš„é€‰æ‹©ç”Ÿæˆæ¸…æ™°ã€è¯¦ç»†çš„ç›®æ ‡æè¿°ã€‚",
            allowed_tools=[],
            max_turns=1,
            cwd=cwd,
        ),
    ):
        if isinstance(message, AssistantMessage):
            for block in message.content:
                if hasattr(block, "text"):
                    summary_text += block.text

    console.print(Panel(summary_text, title="æ˜ç¡®åçš„ç›®æ ‡", border_style="green"))

    # Phase 4: Confirm or edit
    confirm = await ask_select(
        "ç¡®è®¤è¿™ä¸ªç›®æ ‡ï¼Ÿ",
        ["ç¡®è®¤", "éœ€è¦ä¿®æ”¹"],
        allow_custom=False,
    )

    if confirm == "éœ€è¦ä¿®æ”¹":
        edited = await questionary.text(
            "è¯·è¾“å…¥ä¿®æ”¹åçš„ç›®æ ‡æè¿°:",
            style=custom_style,
        ).ask_async()
        if edited:
            summary_text = edited

    # Phase 5: Configure metrics
    metrics_config, eval_config, _ = await clarify_metrics(summary_text)

    # Build goal.md
    goal_content = generate_goal_md(
        initial_prompt=initial_prompt,
        summary_text=summary_text,
        metrics_config=metrics_config,
        eval_config=eval_config,
        qa_text=f"## æ¢ç´¢åˆ†æ\n\n{exploration_result}",
        answers_text=f"ç”¨æˆ·é€‰æ‹©: {selection}",
    )

    # Write to goal.md
    write_goal(goal_content, cwd)
    console.print("\n[green]âœ“ Goal saved to .ralph/goal.md[/green]")

    return goal_content


async def clarify_requirements_v2(
    initial_prompt: str,
    cwd: str = ".",
    mode: str = "auto",
) -> str:
    """
    Unified clarification entry point that chooses the best mode.

    Args:
        initial_prompt: The user's initial request
        cwd: Working directory
        mode: "auto" | "ask" | "explore"
            - auto: Automatically choose based on request clarity
            - ask: Use traditional Q&A mode
            - explore: Use explore+propose mode

    Returns:
        The clarified goal content
    """
    if mode == "ask":
        return await clarify_requirements(initial_prompt, cwd)
    elif mode == "explore":
        return await explore_and_propose(initial_prompt, cwd)
    else:
        # Auto mode: detect based on keywords
        vague_indicators = [
            "ç ”ç©¶", "æ¢ç´¢", "çœ‹çœ‹", "æƒ³æƒ³",
            "å¯èƒ½", "ä¹Ÿè®¸", "ä¸ç¡®å®š",
            "æ€ä¹ˆåš", "ä»€ä¹ˆæ–¹æ³•", "æœ‰ä»€ä¹ˆ",
            "research", "explore", "investigate",
            "could", "might", "maybe",
            "how to", "what if", "possibilities",
        ]

        is_vague = any(indicator in initial_prompt.lower() for indicator in vague_indicators)

        if is_vague:
            console.print("[dim]æ£€æµ‹åˆ°æ¨¡ç³Šéœ€æ±‚ï¼Œä½¿ç”¨æ¢ç´¢+æè®®æ¨¡å¼[/dim]")
            return await explore_and_propose(initial_prompt, cwd)
        else:
            console.print("[dim]éœ€æ±‚ç›¸å¯¹æ˜ç¡®ï¼Œä½¿ç”¨ä¼ ç»Ÿ Q&A æ¨¡å¼[/dim]")
            return await clarify_requirements(initial_prompt, cwd)
