"""
Requirements clarification module.

Clarifies user requirements through two-phase interactive Q&A:
- Phase 1: Agent generates questions as JSON
- Phase 2: Our code presents questions via Rich prompts
- Phase 3: Answers fed back to agent for final output

AskUserQuestion doesn't work through the SDK (interactive form can't render
in piped stdin/stdout mode). Instead we use a two-phase approach.
"""

import json
import re
from claude_agent_sdk import ClaudeAgentOptions
from rich.console import Console
from rich.panel import Panel
from .interactive import ask_user_interactive, CYAN, BOLD, DIM, YELLOW, GREEN, GRAY, RESET

from .metrics import (
    AutomationLevel,
    EvalConfig,
    MetricDefinition,
    MetricsConfig,
    MetricType,
    TaskCategory,
    get_default_metrics,
)
from .logger import log_tool_call, stream_query
from .pool import init_ralph_dir, write_goal
from .prompts import CLARIFIER_SYSTEM_PROMPT, CLARIFIER_V2_SYSTEM_PROMPT, CLARIFIER_V2_EXPLORE_PROMPT

console = Console()


# =============================================================================
# Two-Phase Q&A: Agent generates questions, Rich presents them
# =============================================================================

def _ask_user_rich(questions_json: list[dict]) -> dict[str, str]:
    """Present questions to user via interactive terminal selector.

    Uses arrow-key navigation with highlight bar, direct typing for free text,
    and collapsed confirmation view. Falls back to simple input() on non-Unix.

    Args:
        questions_json: List of question dicts with format:
            [{"question": "...", "options": ["A", "B", "C"]}, ...]

    Returns:
        Dict mapping question text to user's answer.
    """
    return ask_user_interactive(questions_json)


def _format_answers_for_prompt(answers: dict[str, str]) -> str:
    """Format collected answers as text for feeding back to agent."""
    lines = []
    for q, a in answers.items():
        lines.append(f"Q: {q}")
        lines.append(f"A: {a}")
    return "\n".join(lines)


# =============================================================================
# Parsing Functions
# =============================================================================

def parse_metrics(text: str) -> list[dict]:
    """Parse AI-generated metrics from text."""
    metrics = []
    blocks = re.split(r'\n?METRIC:\s*', text)

    for block in blocks[1:]:
        metric = {}

        name_match = re.match(r'^(\S+)', block)
        if name_match:
            metric['name'] = name_match.group(1).strip()

        type_match = re.search(r'TYPE:\s*(\w+)', block, re.IGNORECASE)
        if type_match:
            metric['type'] = type_match.group(1).lower()

        target_match = re.search(r'TARGET:\s*(.+?)(?=\n|$)', block, re.IGNORECASE)
        if target_match:
            metric['target'] = target_match.group(1).strip()

        why_match = re.search(r'WHY:\s*(.+?)(?=\n|$)', block, re.IGNORECASE)
        if why_match:
            metric['why'] = why_match.group(1).strip()

        measure_match = re.search(r'MEASURE:\s*(.+?)(?=\n|$)', block, re.IGNORECASE)
        if measure_match:
            metric['how_to_measure'] = measure_match.group(1).strip()

        auto_match = re.search(r'AUTOMATION:\s*(\w+)', block, re.IGNORECASE)
        if auto_match:
            metric['automation'] = auto_match.group(1).lower()

        proxy_match = re.search(r'PROXY:\s*(.+?)(?=\n|$)', block, re.IGNORECASE)
        if proxy_match:
            metric['proxy_metric'] = proxy_match.group(1).strip()

        batch_match = re.search(r'BATCH:\s*(.+?)(?=\n|$)', block, re.IGNORECASE)
        if batch_match:
            metric['batch_suggestion'] = batch_match.group(1).strip()

        if metric.get('name'):
            metrics.append(metric)

    return metrics


def _extract_json(text: str) -> dict | None:
    """Extract a JSON object from text."""
    fence_match = re.search(r"```(?:json)?\s*\n(\{.*?\})\s*\n```", text, re.DOTALL)
    if fence_match:
        try:
            return json.loads(fence_match.group(1))
        except json.JSONDecodeError:
            pass

    brace_start = text.find("{")
    brace_end = text.rfind("}")
    if brace_start != -1 and brace_end > brace_start:
        try:
            return json.loads(text[brace_start:brace_end + 1])
        except json.JSONDecodeError:
            pass

    return None


# =============================================================================
# Base Metrics (always included)
# =============================================================================

BASE_METRICS = [
    {
        "name": "runs_without_error",
        "type": "hard",
        "target": "pass",
        "why": "åŸºç¡€è¦æ±‚ï¼šä»£ç èƒ½æ­£å¸¸è¿è¡Œ",
        "automation": "auto",
    },
    {
        "name": "basic_functionality",
        "type": "hard",
        "target": "pass",
        "why": "åŸºç¡€è¦æ±‚ï¼šæ ¸å¿ƒåŠŸèƒ½æ­£å¸¸",
        "automation": "auto",
    },
]


# =============================================================================
# Main Clarification Functions
# =============================================================================

async def clarify_metrics(goal_description: str, cwd: str = ".", verbose: bool = False) -> tuple[MetricsConfig, EvalConfig, dict]:
    """
    Clarify success metrics through two-phase interactive Q&A.

    Phase 1: Agent generates questions as JSON
    Phase 2: Our code presents questions via Rich prompts
    Phase 3: Agent generates metrics config based on answers

    Args:
        goal_description: The goal description
        cwd: Working directory
        verbose: Show detailed tool calls and thinking

    Returns:
        Tuple of (MetricsConfig, EvalConfig, all_answers)
    """
    console.print(f"\n[bold cyan]Success Metrics Configuration[/bold cyan]\n")

    # Phase 1: Agent generates questions
    questions_prompt = f"""å¸®ç”¨æˆ·é…ç½®é¡¹ç›®çš„æˆåŠŸæŒ‡æ ‡ã€‚

é¡¹ç›®æè¿°ï¼š
{goal_description}

è¯·ç”Ÿæˆéœ€è¦é—®ç”¨æˆ·çš„é—®é¢˜ï¼Œè¾“å‡º JSON æ ¼å¼ï¼š
```json
{{
  "questions": [
    {{
      "question": "é—®é¢˜æ–‡æœ¬",
      "options": ["é€‰é¡¹1", "é€‰é¡¹2", "é€‰é¡¹3"]
    }}
  ]
}}
```

å¿…é¡»åŒ…å«çš„é—®é¢˜ï¼š
1. é¡¹ç›®ç”¨é€”ï¼šç”Ÿäº§éƒ¨ç½² / ç ”ç©¶å®éªŒ / å­¦ä¹ æ¢ç´¢ / åŸå‹éªŒè¯
2. è¯„ä¼°æ¨¡å¼ï¼šå…¨è‡ªåŠ¨(æœ‰benchmark) / åŠè‡ªåŠ¨(ä»£ç†æŒ‡æ ‡+äººå·¥ç¡®è®¤) / äººå·¥ä¸ºä¸»(çœŸå®ç¯å¢ƒæµ‹è¯•)

æ ¹æ®é¡¹ç›®ç‰¹ç‚¹ï¼Œå†åŠ  1-2 ä¸ªé’ˆå¯¹æ€§çš„æŠ€æœ¯çº¦æŸé—®é¢˜ï¼ˆå¦‚å»¶è¿Ÿã€å‡†ç¡®ç‡ã€æˆæœ¬ç­‰ï¼‰ã€‚

åªè¾“å‡º JSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚
"""

    sr = await stream_query(
        prompt=questions_prompt,
        options=ClaudeAgentOptions(
            system_prompt="ä½ æ˜¯ä¸€ä¸ªå¸®åŠ©é…ç½®é¡¹ç›®è¯„ä¼°æŒ‡æ ‡çš„åŠ©æ‰‹ã€‚åªè¾“å‡º JSON æ ¼å¼çš„é—®é¢˜åˆ—è¡¨ã€‚",
            max_turns=1,
            cwd=cwd,
        ),
        agent_name="clarifier",
        emoji="ğŸ“",
        cwd=cwd,
        verbose=verbose,
        status_message="Generating metrics questions...",
    )
    questions_text = sr.text

    # Parse questions and present to user
    questions_json = _extract_json(questions_text)
    questions_list = questions_json.get("questions", []) if questions_json else []

    if not questions_list:
        # Fallback: use hardcoded questions
        questions_list = [
            {"question": "é¡¹ç›®ç”¨é€”æ˜¯ä»€ä¹ˆï¼Ÿ", "options": ["ç”Ÿäº§éƒ¨ç½²", "ç ”ç©¶å®éªŒ", "å­¦ä¹ æ¢ç´¢", "åŸå‹éªŒè¯"]},
            {"question": "è¯„ä¼°æ¨¡å¼åå¥½ï¼Ÿ", "options": ["å…¨è‡ªåŠ¨(æœ‰benchmark)", "åŠè‡ªåŠ¨(ä»£ç†æŒ‡æ ‡+äººå·¥ç¡®è®¤)", "äººå·¥ä¸ºä¸»(çœŸå®ç¯å¢ƒæµ‹è¯•)"]},
        ]

    # Phase 2: Present questions via Rich prompts
    answers = _ask_user_rich(questions_list)
    answers_text = _format_answers_for_prompt(answers)

    if verbose:
        console.print(f"[dim]Collected {len(answers)} answers[/dim]")

    # Phase 3: Agent generates metrics based on answers
    metrics_prompt = f"""æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆè¯„ä¼°æŒ‡æ ‡é…ç½®ã€‚

é¡¹ç›®æè¿°ï¼š
{goal_description}

ç”¨æˆ·å›ç­”ï¼š
{answers_text}

è¯·è¾“å‡º JSON æ ¼å¼çš„è¯„ä¼°æŒ‡æ ‡é…ç½®ï¼š
```json
{{
  "purpose": "é¡¹ç›®ç”¨é€”",
  "eval_mode": "è¯„ä¼°æ¨¡å¼",
  "test_frequency": "æµ‹è¯•é¢‘ç‡ï¼ˆå¦‚é€‚ç”¨ï¼‰",
  "batch_preference": "æµ‹è¯•å®‰æ’åå¥½ï¼ˆå¦‚é€‚ç”¨ï¼‰",
  "category": "algorithm|web|api|cli|library|general",
  "metrics": [
    {{
      "name": "metric_name_in_snake_case",
      "type": "hard|soft|subjective",
      "target": "ç›®æ ‡å€¼",
      "why": "ä¸ºä»€ä¹ˆé‡è¦",
      "automation": "auto|manual|hybrid",
      "proxy_metric": "ä»£ç†æŒ‡æ ‡ï¼ˆå¦‚é€‚ç”¨ï¼‰",
      "batch_suggestion": "æ‰¹é‡æµ‹è¯•å»ºè®®ï¼ˆå¦‚é€‚ç”¨ï¼‰"
    }}
  ]
}}
```

æ³¨æ„ï¼š
- metrics æ•°ç»„åº”åŒ…å« 2-4 ä¸ªæŒ‡æ ‡
- æ ¹æ®ç”¨æˆ·çš„å›ç­”é€‰æ‹©åˆé€‚çš„ categoryã€typeã€automation ç­‰
- åªè¾“å‡º JSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—
"""

    sr = await stream_query(
        prompt=metrics_prompt,
        options=ClaudeAgentOptions(
            system_prompt="ä½ æ˜¯ä¸€ä¸ªå¸®åŠ©é…ç½®é¡¹ç›®è¯„ä¼°æŒ‡æ ‡çš„åŠ©æ‰‹ã€‚æ ¹æ®ç”¨æˆ·çš„å›ç­”ç”Ÿæˆç»“æ„åŒ–çš„æŒ‡æ ‡é…ç½®ã€‚åªè¾“å‡º JSONã€‚",
            max_turns=1,
            cwd=cwd,
        ),
        agent_name="clarifier",
        emoji="ğŸ“",
        cwd=cwd,
        verbose=verbose,
        status_message="Generating metrics config...",
    )
    result_text = sr.text

    # Parse JSON output
    all_answers = {"goal": goal_description}
    json_obj = _extract_json(result_text)

    if json_obj:
        all_answers["purpose"] = json_obj.get("purpose", "")
        all_answers["eval_mode"] = json_obj.get("eval_mode", "")
        if json_obj.get("test_frequency"):
            all_answers["test_frequency"] = json_obj["test_frequency"]
        if json_obj.get("batch_preference"):
            all_answers["batch_preference"] = json_obj["batch_preference"]
        dynamic_metrics = json_obj.get("metrics", [])
    else:
        # Fallback: try to parse metrics from text format
        all_answers["purpose"] = "åŸå‹éªŒè¯"
        all_answers["eval_mode"] = "å…¨è‡ªåŠ¨"
        dynamic_metrics = parse_metrics(result_text)

    # Build EvalConfig
    eval_mode = all_answers.get("eval_mode", "å…¨è‡ªåŠ¨")
    eval_config = EvalConfig(mode=eval_mode)
    if all_answers.get("test_frequency"):
        eval_config.test_frequency = all_answers["test_frequency"]
    if all_answers.get("batch_preference"):
        eval_config.batch_preference = all_answers["batch_preference"]

    # Combine base + dynamic metrics
    all_metrics = BASE_METRICS + dynamic_metrics

    # Display metrics as vertical cards
    print(f"\n  {BOLD}{GREEN}ç”Ÿæˆçš„è¯„ä¼°æŒ‡æ ‡{RESET}\n")

    for m in all_metrics:
        name = m.get('name', '')
        mtype = m.get('type', '')
        auto_label = {"auto": "è‡ªåŠ¨", "manual": "äººå·¥", "hybrid": "æ··åˆ"}.get(
            m.get("automation", "auto"), "è‡ªåŠ¨"
        )
        auto_color = {"auto": GREEN, "manual": YELLOW, "hybrid": CYAN}.get(
            m.get("automation", "auto"), GREEN
        )
        target = m.get('target', '')
        why = m.get('why', '')

        tags = f"{DIM}{mtype} Â· {auto_color}{auto_label}{RESET}"
        print(f"  {CYAN}â•­{RESET} {BOLD}{CYAN}{name}{RESET}  {tags}")
        if target:
            print(f"  {CYAN}â”‚{RESET} ç›®æ ‡: {YELLOW}{target}{RESET}")
        if why:
            print(f"  {CYAN}â”‚{RESET} {DIM}{why}{RESET}")
        print(f"  {CYAN}â•°{RESET}\n")

    # Convert to MetricsConfig
    category_str = json_obj.get("category", "general") if json_obj else "general"
    try:
        category = TaskCategory(category_str)
    except ValueError:
        category = TaskCategory.GENERAL
    metrics_config = MetricsConfig(category=category, eval_config=eval_config)

    for m in all_metrics:
        metric_type = MetricType(m.get("type", "soft"))
        automation = AutomationLevel(m.get("automation", "auto"))

        metric_def = MetricDefinition(
            name=m["name"],
            type=metric_type,
            description=m.get("why", ""),
            target=m.get("target"),
            automation=automation,
            proxy_metric=m.get("proxy_metric"),
            batch_suggestion=m.get("batch_suggestion"),
        )

        if metric_type == MetricType.HARD:
            metrics_config.hard_constraints.append(metric_def)
        elif metric_type == MetricType.SOFT:
            metrics_config.soft_targets.append(metric_def)
        else:
            metrics_config.subjective_criteria.append(metric_def)

    console.print("\n[green]âœ“ Metrics configured[/green]")
    return metrics_config, eval_config, all_answers


def generate_goal_md(
    initial_prompt: str,
    summary_text: str,
    metrics_config: MetricsConfig,
    eval_config: EvalConfig,
    qa_text: str = "",
    answers_text: str = "",
) -> str:
    """Generate complete goal.md content."""
    lines = ["# Goal", ""]

    # Original request
    lines.append("## Original Request")
    lines.append(initial_prompt)
    lines.append("")

    # Q&A section (if any)
    if qa_text and answers_text:
        lines.append("## Clarification")
        lines.append("")
        lines.append("### Questions")
        lines.append(qa_text)
        lines.append("")
        lines.append("### Answers")
        lines.append(answers_text)
        lines.append("")

    # Clarified description
    lines.append("## Clarified Description")
    lines.append(summary_text)
    lines.append("")

    # Evaluation Mode section
    if eval_config.needs_user_testing():
        lines.append("## Evaluation Mode")
        lines.append("")
        lines.append(f"- **æµ‹è¯•æ¨¡å¼**: {eval_config.mode}")
        if eval_config.test_frequency:
            lines.append(f"- **æµ‹è¯•é¢‘ç‡**: {eval_config.test_frequency}")
        if eval_config.batch_preference:
            lines.append(f"- **æµ‹è¯•å®‰æ’**: {eval_config.batch_preference}")
        lines.append("")

    # Success Metrics
    lines.append("## Success Metrics")
    lines.append("")

    # Hard constraints
    if metrics_config.hard_constraints:
        lines.append("### Hard Constraints (must pass)")
        for m in metrics_config.hard_constraints:
            auto_tag = "[auto]" if m.automation == AutomationLevel.AUTO else "[manual]"
            lines.append(f"- [ ] **{m.name}** {auto_tag}: {m.target or 'pass'} - {m.description}")
        lines.append("")

    # Soft targets
    if metrics_config.soft_targets:
        lines.append("### Performance Targets")
        lines.append("| Metric | Target | Automation | Proxy |")
        lines.append("|--------|--------|------------|-------|")
        for m in metrics_config.soft_targets:
            proxy = m.proxy_metric or "-"
            lines.append(f"| {m.name} | {m.target or 'N/A'} | {m.automation.value} | {proxy} |")
        lines.append("")

    # Subjective criteria
    if metrics_config.subjective_criteria:
        lines.append("### Quality Criteria (AI-evaluated)")
        for m in metrics_config.subjective_criteria:
            lines.append(f"- **{m.name}**: {m.description}")
        lines.append("")

    # Manual testing instructions
    manual_metrics = [
        m for m in metrics_config.all_metrics()
        if m.automation in (AutomationLevel.MANUAL, AutomationLevel.HYBRID)
    ]
    if manual_metrics:
        lines.append("### Manual Testing Instructions")
        for m in manual_metrics:
            if m.batch_suggestion:
                lines.append(f"- **{m.name}**: {m.batch_suggestion}")
        lines.append("")

    return "\n".join(lines)


async def clarify_requirements(initial_prompt: str, cwd: str = ".", verbose: bool = False) -> str:
    """
    Clarify requirements through two-phase interactive Q&A.

    Phase 1: Agent explores codebase and generates questions as JSON
    Phase 2: Our code presents questions via Rich prompts
    Phase 3: Agent generates clarified requirements using answers

    Args:
        initial_prompt: The user's initial feature request
        cwd: Working directory
        verbose: Show detailed tool calls and thinking

    Returns:
        The clarified goal content (also written to goal.md)
    """
    init_ralph_dir(cwd)

    console.print(Panel(f"[bold]Feature Request:[/bold]\n{initial_prompt}", title="Input"))
    console.print("\n[yellow]Analyzing requirements...[/yellow]\n")

    # Phase 1: Agent explores codebase and generates questions
    explore_prompt = f"""User's feature request:
{initial_prompt}

è¯·æŒ‰ä»¥ä¸‹æµç¨‹æ“ä½œï¼š
1. æ¢ç´¢ä»£ç åº“ï¼Œäº†è§£é¡¹ç›®ç»“æ„ã€æŠ€æœ¯æ ˆã€ç°æœ‰æ¨¡å¼
2. æ ¹æ®æ¢ç´¢ç»“æœï¼Œç”Ÿæˆéœ€è¦é—®ç”¨æˆ·çš„æ¾„æ¸…é—®é¢˜

æœ€åè¾“å‡º JSON æ ¼å¼çš„é—®é¢˜åˆ—è¡¨ï¼š
```json
{{
  "codebase_context": "å¯¹ä»£ç åº“çš„ç®€è¦ç†è§£",
  "questions": [
    {{
      "question": "é—®é¢˜æ–‡æœ¬",
      "options": ["é€‰é¡¹1", "é€‰é¡¹2", "é€‰é¡¹3"]
    }}
  ]
}}
```

é—®é¢˜åº”è¯¥å…³æ³¨ï¼šéœ€æ±‚èŒƒå›´ã€ç›®æ ‡ç”¨æˆ·ã€æ ¸å¿ƒåŠŸèƒ½ã€æŠ€æœ¯çº¦æŸã€‚
ç”Ÿæˆ 2-4 ä¸ªé—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜ 2-4 ä¸ªé€‰é¡¹ã€‚
ç¡®ä¿ JSON æ˜¯è¾“å‡ºçš„æœ€åä¸€éƒ¨åˆ†ã€‚
"""

    sr = await stream_query(
        prompt=explore_prompt,
        options=ClaudeAgentOptions(
            system_prompt=CLARIFIER_SYSTEM_PROMPT,
            allowed_tools=[
                "Read", "Glob", "Grep", "LSP",
                "WebFetch", "WebSearch",
            ],
            max_turns=15,
            cwd=cwd,
        ),
        agent_name="clarifier",
        emoji="ğŸ”",
        cwd=cwd,
        verbose=verbose,
        show_tools=True,
    )
    explore_text = sr.text

    # Parse questions and present to user
    questions_json = _extract_json(explore_text)
    codebase_context = questions_json.get("codebase_context", "") if questions_json else ""
    questions_list = questions_json.get("questions", []) if questions_json else []

    if not questions_list:
        # Fallback: use generic questions
        questions_list = [
            {"question": "è¿™ä¸ªåŠŸèƒ½çš„ç›®æ ‡ç”¨æˆ·æ˜¯è°ï¼Ÿ", "options": ["å¼€å‘è€…", "ç»ˆç«¯ç”¨æˆ·", "è¿ç»´äººå‘˜", "æ‰€æœ‰äºº"]},
            {"question": "æ ¸å¿ƒéœ€æ±‚æ˜¯ä»€ä¹ˆï¼Ÿ", "options": ["æ–°åŠŸèƒ½", "æ€§èƒ½ä¼˜åŒ–", "Bugä¿®å¤", "é‡æ„"]},
        ]

    # Phase 2: Present questions via Rich prompts
    console.print("\n[bold cyan]Clarification Questions[/bold cyan]")
    answers = _ask_user_rich(questions_list)
    answers_text = _format_answers_for_prompt(answers)

    # Phase 3: Agent generates clarified requirements using answers
    clarify_prompt = f"""User's feature request:
{initial_prompt}

ä»£ç åº“ä¸Šä¸‹æ–‡ï¼š
{codebase_context}

ç”¨æˆ·å¯¹æ¾„æ¸…é—®é¢˜çš„å›ç­”ï¼š
{answers_text}

è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œç”Ÿæˆ clarified requirementsï¼ˆmarkdown æ ¼å¼ï¼‰ã€‚

è¾“å‡ºè¦æ±‚ï¼š
- Clear, detailed description of what needs to be built
- Scope (what's included)
- Non-goals (what's explicitly NOT included)
- Important context from codebase exploration
- Temporal Topics (éœ€éªŒè¯çš„æ—¶æ•ˆæ€§è¯é¢˜)
"""

    sr = await stream_query(
        prompt=clarify_prompt,
        options=ClaudeAgentOptions(
            system_prompt=CLARIFIER_SYSTEM_PROMPT,
            max_turns=3,
            cwd=cwd,
        ),
        agent_name="clarifier",
        emoji="ğŸ“",
        cwd=cwd,
        verbose=verbose,
        status_message="Generating clarified requirements...",
    )
    summary_text = sr.text

    console.print(Panel(summary_text, title="Clarified Requirements"))

    # Phase 2: Clarify success metrics
    metrics_config, eval_config, _ = await clarify_metrics(summary_text, cwd, verbose=verbose)

    # Build goal.md content
    goal_content = generate_goal_md(
        initial_prompt=initial_prompt,
        summary_text=summary_text,
        metrics_config=metrics_config,
        eval_config=eval_config,
    )

    # Write to goal.md
    write_goal(goal_content, cwd)
    console.print("\n[green]âœ“ Goal saved to .ralph/goal.md[/green]")

    return goal_content


async def quick_clarify(initial_prompt: str, cwd: str = ".") -> str:
    """
    Quick clarification without interactive Q&A.
    Useful for simple, well-defined requests.

    Args:
        initial_prompt: The user's initial feature request
        cwd: Working directory

    Returns:
        The goal content (also written to goal.md)
    """
    init_ralph_dir(cwd)

    # Use general default metrics (category detection moved to agent prompt)
    category = TaskCategory.GENERAL
    metrics_config = get_default_metrics(category)
    eval_config = EvalConfig(mode="å…¨è‡ªåŠ¨")

    goal_content = generate_goal_md(
        initial_prompt=initial_prompt,
        summary_text=initial_prompt,
        metrics_config=metrics_config,
        eval_config=eval_config,
    )

    write_goal(goal_content, cwd)
    console.print(f"\n[green]âœ“ Goal saved to .ralph/goal.md[/green]")
    console.print(f"[dim]Task category: {category.value}, using default metrics[/dim]")

    return goal_content


# =============================================================================
# Clarifier v2: Explore and Propose Mode
# =============================================================================

PROPOSAL_PARSE_PROMPT = """
ä»ä»¥ä¸‹æ¢ç´¢ç»“æœä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ã€‚

æ¢ç´¢ç»“æœï¼š
---
{exploration_result}
---

è¯·æå–å¹¶è¾“å‡ºä»¥ä¸‹ JSON æ ¼å¼ï¼š
```json
{
  "understanding": "å¯¹ç”¨æˆ·éœ€æ±‚çš„ä¸€å¥è¯ç†è§£",
  "proposals": [
    {
      "name": "æ–¹æ¡ˆåç§°",
      "summary": "ä¸€å¥è¯æ¦‚è¿°",
      "pros": ["ä¼˜ç‚¹1", "ä¼˜ç‚¹2"],
      "cons": ["ç¼ºç‚¹1", "ç¼ºç‚¹2"],
      "complexity": "ä½|ä¸­|é«˜",
      "risk": "ä¸»è¦é£é™©"
    }
  ],
  "recommendation": {
    "name": "æ¨èçš„æ–¹æ¡ˆåç§°",
    "reasons": ["åŸå› 1", "åŸå› 2", "åŸå› 3"]
  },
  "temporal_topics": ["éœ€è¦éªŒè¯çš„æ—¶æ•ˆæ€§è¯é¢˜1", "è¯é¢˜2"]
}
```

åªè¾“å‡º JSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚
"""


async def explore_and_propose(initial_prompt: str, cwd: str = ".", verbose: bool = False) -> str:
    """
    Clarifier v2: Explore possible approaches and propose options to user.

    Phase 1: Agent explores codebase and generates proposals as JSON
    Phase 2: Our code presents proposals via Rich prompts
    Phase 3: Agent generates clarified goal based on user's choice

    Args:
        initial_prompt: The user's initial (possibly vague) request
        cwd: Working directory
        verbose: Show detailed tool calls and thinking

    Returns:
        The clarified goal content (also written to goal.md)
    """
    init_ralph_dir(cwd)

    console.print(Panel(
        f"[bold]ç”¨æˆ·éœ€æ±‚:[/bold]\n{initial_prompt}",
        title="[cyan]Clarifier v2: æ¢ç´¢+æè®®æ¨¡å¼[/cyan]",
        border_style="cyan",
    ))

    console.print("\n[yellow]æ·±åº¦æ¢ç´¢ä¸­...[/yellow]")
    console.print("[dim]Agent æ­£åœ¨ç ”ç©¶å¯èƒ½çš„å®ç°æ–¹æ¡ˆ...[/dim]\n")

    # Phase 1: Agent explores and generates proposals as JSON
    explore_prompt = CLARIFIER_V2_EXPLORE_PROMPT.format(user_request=initial_prompt) + """

å®Œæˆæ¢ç´¢åï¼Œè¾“å‡º JSON æ ¼å¼çš„æ–¹æ¡ˆæè®®ï¼š
```json
{
  "understanding": "å¯¹ç”¨æˆ·éœ€æ±‚çš„ä¸€å¥è¯ç†è§£",
  "proposals": [
    {
      "name": "æ–¹æ¡ˆåç§°",
      "summary": "ä¸€å¥è¯æ¦‚è¿°",
      "pros": ["ä¼˜ç‚¹1", "ä¼˜ç‚¹2"],
      "cons": ["ç¼ºç‚¹1", "ç¼ºç‚¹2"]
    }
  ],
  "follow_up_questions": [
    {
      "question": "éœ€è¦è¿›ä¸€æ­¥äº†è§£çš„é—®é¢˜",
      "options": ["é€‰é¡¹1", "é€‰é¡¹2", "é€‰é¡¹3"]
    }
  ]
}
```

ç¡®ä¿ JSON æ˜¯è¾“å‡ºçš„æœ€åä¸€éƒ¨åˆ†ã€‚
"""

    sr = await stream_query(
        prompt=explore_prompt,
        options=ClaudeAgentOptions(
            system_prompt=CLARIFIER_V2_SYSTEM_PROMPT,
            allowed_tools=[
                "Read", "Glob", "Grep", "LSP",
                "WebFetch", "WebSearch", "Task",
            ],
            max_turns=25,
            cwd=cwd,
        ),
        agent_name="clarifier_v2",
        emoji="ğŸ”",
        cwd=cwd,
        verbose=verbose,
        show_tools=True,
    )
    explore_text = sr.text

    # Parse proposals and present to user
    proposals_json = _extract_json(explore_text)

    if proposals_json and proposals_json.get("proposals"):
        understanding = proposals_json.get("understanding", "")
        proposals = proposals_json["proposals"]

        if understanding:
            console.print(f"\n[bold]ç†è§£:[/bold] {understanding}\n")

        # Present proposals as a question
        proposal_question = {
            "question": "è¯·é€‰æ‹©ä¸€ä¸ªå®ç°æ–¹æ¡ˆï¼š",
            "options": [
                {"label": f"{p['name']}: {p['summary']}"} for p in proposals
            ],
        }
        answers = _ask_user_rich([proposal_question])

        # Also ask follow-up questions if any
        follow_ups = proposals_json.get("follow_up_questions", [])
        if follow_ups:
            console.print("\n[bold cyan]Follow-up Questions[/bold cyan]")
            follow_up_answers = _ask_user_rich(follow_ups)
            answers.update(follow_up_answers)
    else:
        # Fallback: generic question
        answers = _ask_user_rich([
            {"question": "ä½ å¯¹è¿™ä¸ªéœ€æ±‚æœ‰ä»€ä¹ˆå…·ä½“çš„åå¥½ï¼Ÿ", "options": ["ç®€å•å®ç°", "å®Œæ•´æ–¹æ¡ˆ", "æœ€ä½³å®è·µ"]},
        ])

    answers_text = _format_answers_for_prompt(answers)

    # Phase 3: Agent generates clarified goal based on user's choice
    goal_prompt = f"""ç”¨æˆ·éœ€æ±‚ï¼š
{initial_prompt}

æ¢ç´¢ç»“æœå’Œæ–¹æ¡ˆï¼š
{explore_text[:3000]}

ç”¨æˆ·çš„é€‰æ‹©å’Œå›ç­”ï¼š
{answers_text}

è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œç”Ÿæˆæ˜ç¡®çš„ç›®æ ‡æè¿°ï¼ˆmarkdown æ ¼å¼ï¼‰ï¼ŒåŒ…å«ï¼š
- Clarified Description
- Scope
- Non-goals
- Technical Approach
- Risks and Mitigations
"""

    sr = await stream_query(
        prompt=goal_prompt,
        options=ClaudeAgentOptions(
            system_prompt=CLARIFIER_V2_SYSTEM_PROMPT,
            max_turns=3,
            cwd=cwd,
        ),
        agent_name="clarifier_v2",
        emoji="ğŸ“",
        cwd=cwd,
        verbose=verbose,
        status_message="Generating goal summary...",
    )
    summary_text = sr.text

    console.print(Panel(summary_text, title="æ˜ç¡®åçš„ç›®æ ‡", border_style="green"))

    # Configure metrics
    metrics_config, eval_config, _ = await clarify_metrics(summary_text, cwd, verbose=verbose)

    # Build goal.md
    goal_content = generate_goal_md(
        initial_prompt=initial_prompt,
        summary_text=summary_text,
        metrics_config=metrics_config,
        eval_config=eval_config,
    )

    # Write to goal.md
    write_goal(goal_content, cwd)
    console.print("\n[green]âœ“ Goal saved to .ralph/goal.md[/green]")

    return goal_content


async def clarify_requirements_v2(
    initial_prompt: str,
    cwd: str = ".",
    mode: str = "auto",
    verbose: bool = False,
) -> str:
    """
    Unified clarification entry point that chooses the best mode.

    Args:
        initial_prompt: The user's initial request
        cwd: Working directory
        mode: "auto" | "ask" | "explore"
            - auto: Automatically choose based on request clarity
            - ask: Use traditional Q&A mode
            - explore: Use explore+propose mode
        verbose: Show detailed tool calls and thinking

    Returns:
        The clarified goal content
    """
    if mode == "ask":
        return await clarify_requirements(initial_prompt, cwd, verbose=verbose)
    elif mode == "explore":
        return await explore_and_propose(initial_prompt, cwd, verbose=verbose)
    else:
        # Auto mode: detect based on keywords
        vague_indicators = [
            "ç ”ç©¶", "æ¢ç´¢", "çœ‹çœ‹", "æƒ³æƒ³",
            "å¯èƒ½", "ä¹Ÿè®¸", "ä¸ç¡®å®š",
            "æ€ä¹ˆåš", "ä»€ä¹ˆæ–¹æ³•", "æœ‰ä»€ä¹ˆ",
            "research", "explore", "investigate",
            "could", "might", "maybe",
            "how to", "what if", "possibilities",
        ]

        is_vague = any(indicator in initial_prompt.lower() for indicator in vague_indicators)

        if is_vague:
            console.print("[dim]æ£€æµ‹åˆ°æ¨¡ç³Šéœ€æ±‚ï¼Œä½¿ç”¨æ¢ç´¢+æè®®æ¨¡å¼[/dim]")
            return await explore_and_propose(initial_prompt, cwd, verbose=verbose)
        else:
            console.print("[dim]éœ€æ±‚ç›¸å¯¹æ˜ç¡®ï¼Œä½¿ç”¨ä¼ ç»Ÿ Q&A æ¨¡å¼[/dim]")
            return await clarify_requirements(initial_prompt, cwd, verbose=verbose)
